[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Workshop",
    "section": "",
    "text": "Welcome\nThank you for checking out this workshop! This page contains a brief overview of the goals, structure, and prerequisites. Although, much of the text is written like a book, this is meant to be an active workshop with RStudio open and fingers at our keyboards. We will reference the narrative text occasionally, but mostly its purpose is a solid reference after the fact to fully flesh out the practical principles we will be talking about."
  },
  {
    "objectID": "index.html#goals",
    "href": "index.html#goals",
    "title": "Data Science Workshop",
    "section": "Goals",
    "text": "Goals\nAs an open-source programming language, there is an abundance of approaches to doing data science in R. It is a wide open landscape ready for your exploration! But — to extend this metaphor to its breaking point — it isn’t just a flat landscape, where each spot is as fertile as the next. It is more complicated. Its closer to a fitness landscape where there are hidden local and global optima 1. You are encouraged to explore that space for yourself – find the high points and low points – what works best for you and what doesn’t2. But it is my hope that this workshop will serve as a useful initialization of your search of that complex adaptive landscape. And that along the way you will gain tools which will help you dig out of those valleys and climb to the top of those peaks. I have the following high-level goals for this workshop:\n\nFoster an appreciation of the tidyverse and its underlying principles. Tools in R, and many other programming languages, are constantly evolving and that can make learning new tools seem futile. Especially if you already have something functional with a different set of tools. But the tidyverse, is likely to stick around, and it also represents something beyond a set of tools. Working with the tidyverse encourages good coding practices that can help us write more useful and readable code.\nPersuade you to go beyond a graph that merely gets the job done and toward a version that most clearly communicates the story behind your data. There are many ways to represent data in graphical form. Each time we create a figure we are making dozens – or maybe hundreds – of choices. Some choices are passive3, and some are active4, and some may have little effect5, but I hope to convince you that spending some additional time iterating figures that you will share is well worth it.\nEncourage best practices to ensure computational reproducibility. Reproducibility is not just necessary for good science – it will save you and your collaborators a lot of time in the long run. Making sure your code is reproducible is not as difficult as it may seem."
  },
  {
    "objectID": "index.html#structure",
    "href": "index.html#structure",
    "title": "Data Science Workshop",
    "section": "Structure",
    "text": "Structure\nThere are three parts to this workshop. But each is designed to be self-contained – so you may pick-and-choose to attend any or all of these workshops. The three parts are as follows:\n\nComputational reproducibility\nPrinciples of the tidyverse and advanced techniques\nData visualization for presentations and publications"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Data Science Workshop",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis workshop assumes several things6:\n\nYou have R and RStudio installed on your machine and you come ready to follow along with some coding.\nYou have some basic familiarity with coding in R. For example, you know the difference between a vector and a data frame.\nYou are able to do some simple tasks in R – like creating an object, getting the mean of a vector, or importing a csv file.\n\n\n\n\n\n\n\nGood news if you are starting from zero\n\n\n\nYou can meet the above criteria in less than a day!\nThere are several great resources on the internet that will walk you through downloading R and R Studio and give you the basics. You will not need much experience at all for our workshop – just a familiarity with the basics. Due to our limited time, and the wealth of resources covering step zero, we will start at step one and hit the ground running.\n\nResources if you are starting out – or need a quick refresh\n\nR for Graduate Students – very accessible introduction to R & the tidyverse\nR for Data Science – a wonderfully thorough and useful book that emphasizes the tidyverse"
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Data Science Workshop",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe following workshop contains my opinions on learning R and programming in general. I am no expert – so please feel free to disagree with me on anything. In general, this workshop is designed to give you some exposure to a few useful topics, but it is by necessity incomplete. Very little, if any, of this workshop is uniquely mine. Instead, over the years, I have compiled a bunch of useful nuggets from other places. And I have packaged those nuggets here to fit within the scope this workshop. But, I strongly encourage you to check out those alternative7 resources. They are linked below and throughout this document. Therefore, I hope this tutorial will be useful, if through nothing else, as a portal to more useful parts of the internet.\n\n\n\n\n\n\nAlternative resources\n\n\n\n\nR for Data Science – a wonderfully thorough and useful book that emphasizes the tidyverse\nR for Graduate Students – very accessible introduction to R & the tidyverse\nFundamentals of Data Visualization – “A guide to making visualizations that accurately reflect the data, tell a story, and look professional.” by Claus O. Wilke. This book is great, because it is not at all about programming, but just how to make the best data visualizations.\nAdvanced R – R with the nitty-gritty details for the super nerds out there.\nggplot2 book – detailed introduction to plotting with ggplot2\nLearning Statistics with R – great book with an emphasis on stats\nR Markdown: The Definitive Guide – great overview of the features of R Markdown\nR Markdown Cookbook – additional R Markdown guide\nHappy Git and GitHub for the useR – resource for version control\nNick Gotelli’s Computational Biology – a wonderful course on R taught by Nick Gotelli at UVM."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Data Science Workshop",
    "section": "",
    "text": "↩︎\nI am not claiming to have found the global optimum of data science. I promise I would share the secret if I had it.↩︎\nlike keeping the default theme – yikes!↩︎\nShould I use a box plot, strip chart, or violin plot?↩︎\nI confess I have spent way to much time trying to find the most pleasing shade of purple.↩︎\nThese prerequisites are intentionally designed to be a relatively low bar. See the green box if you don’t meet these prerequisites.↩︎\nread: better↩︎"
  },
  {
    "objectID": "repro.html#practical-reproducibility",
    "href": "repro.html#practical-reproducibility",
    "title": "3  Reproducibility",
    "section": "3.1 Practical reproducibility",
    "text": "3.1 Practical reproducibility\nMuch of the advice on computational reproducibility is somewhat abstract5. That is, in part, the nature of the beast. Each project represents its own unique challenges. But it is also because the advice is often made for a broad range of projects in all sorts of different programming languages on everything from model simulations to genome assembly, all the way to creating programmatic tools for others. In contrast, this workshop will focus on a narrower set of tasks related to statistical programming in R6. In other words, the type of programming where you have some raw data generated elsewhere (e.g., enzyme activity or species abundance data) that you are going to preform some sort of analysis on it (e.g., normalization and a significance test), and then make figures.\n\n\n\n\n\n\nTL;DR - Practical tips for computational reproducibility 7\n\n\n\n\n\n\nR & RStudio specific tips\n\nUse the Projects feature in R.\nStart with a blank slate! Do not save .RData on exit, and do not restore .RData on open. You can change this default behavior in RStudio in the Global Options.\nUse tidyverse packages! See the tidyverse chapter of this workshop.\nTake advantage of the templates for RMarkdown and Quarto if you are learning how to make them.\n\n\n\nRepository tips\n\nUse a consistent directory structure. You can save this structure as a template!\nUse sub-directories. Favor an ordered highly nested directory structure, over a directory with dozens of files with long and repetitive names.8\nAdd number prefixes to your scripts (and possibly directories), so it is clear which order they must be run 01_normalize_data.R.\nTry a subdirectory structure that is symmetrical – this makes it easier to know where the relevant info is saved. For example, the data from data/raw/pheno/2023-07-04_data.csv could be analyzed in a script in src/pheno/01_anova.R, and the output could be saved in output/pheno/anova_results.rds and the corresponding figure saved in output/figs/pheno/boxplot.pdf. In each case, there is a pheno/ subdirectory.\n\n\n\nScript writing\n\nEvery script should be able to run without errors from top to bottom (i.e., in R, source(file_name.R) or clicking the source button in RStudio should always work when you save a file).\nWhen you are using multiple packages with overlapping function names, the order that you load the libraries can matter. If you have this make sure you can\nAvoid magic numbers. Unexplained numbers within the middle of the script that could easily be replaced with assigning the number to a named variable at the top of the script.\nThe order of each script should make sense and be consistent (e.g., description, load packages, load data, manipulate data, save data). If you find yourself violating this rule, either by loading packages later in a script or multiple saves of data intermediates within a file, it may make sense to split up the script.\nFavor small scripts that are focused on a single task, over big scripts that do many things.\nYou should be able to run every script with a completely clean global environment.\nDevelop a consistent coding style (e.g., snake_case, indents, comments). See the tidyverse style guide for a good set of rules.\nYou should be able to clone the parent directory of the project and run the scripts without alteration on any machine.\n\n\n\nData handling\n\nAvoid any manual manipulation of data (i.e., don’t mess around copy-and-pasting or editing raw data. Change it reproducibly with code.\nSave output automatically by writing it into the code (e.g., saveRDS(), readr::write_csv(), ggsave()).\nSave intermediate data. If you are starting with a big data set, it is nice save that intermediate so a collaborator (or you in the future, or some random researcher on the internet), can re-do an intermediate step rather than begin from raw data. If they want to know how different you results would look normalized your data in a different way\n\n\n\nRandom pet-peeves\n\nDon’t copy and paste output into R scripts. If you need to save an output table, then write it to a csv or save it as an rds file. If you need quick access to some intermedite info then use RMarkdown or Quarto to create html reports.\nDon’t include anything that isn’t necessary to your final product in your code.\nOpt for long and explicit variable/function names over short and implicit names.\nDo not save install.packages(\"some_package\") in your script – even if it is commented out.9\n\n\n\nVersion control\n\nCommit and push relatively often. This makes your commit history a useful record the changes you have made. It also makes it less likely that you will run into issues pushing and pulling. Or at least less traumatic if you do run into issues.\nUse RStudio’s built in version control tools. There are easy ways to interact with Git and GitHub with the RStudio IDE.\nAlways pull first – just in case your local state is a little behind.\nDon’t commit large files (e.g., raw data or large pdf figures) to version control. The software usually has limits."
  },
  {
    "objectID": "repro.html#ten-simple-rules",
    "href": "repro.html#ten-simple-rules",
    "title": "3  Reproducibility",
    "section": "3.2 Ten simple rules",
    "text": "3.2 Ten simple rules\nThe tips outlined above are a useful and specific starting point10. But rather than rely solely on my proclivities, let’s instead adopt these simple rules from Sandve et al. (2013). Over the course of this workshop, we will look at some specific coding practices and think about how they may violate, or adhere to, one (or more) of these rules. The additional benefit of adopting these rules is that they are easy enough to apply to other types of projects. It is worth reading in full.\n\n\n\n\n\n\nTen simple rules for reproducible computational research\n\n\n\n\nFor every result, keep track of how it was produced\nAvoid manual data manipulation steps\nArchive the exact versions of all external programs used\nVersion control all custom scripts\nRecord all intermediate results, when possible in standardized formats\nFor analyses that include randomness, note underlying random seeds\nAlways store raw data behind plots\nGenerate hierarchical analysis output, allowing layers of increasing detail to be inspected\nConnect textual statements to underlying results\nProvide public access to scripts, runs, and results"
  },
  {
    "objectID": "repro.html#opening-in-rstudio",
    "href": "repro.html#opening-in-rstudio",
    "title": "3  Reproducibility",
    "section": "3.3 Opening in RStudio",
    "text": "3.3 Opening in RStudio\nOkay, let’s begin by opening up RStudio11. Do you have objects already in your Global Environment? Is the console full of code you ran last time? Or do you always keep RStudio running, because you are worried about loosing the results you finally managed to get, and you need to do more stuff later?\nI know people that do great work in R and live their lives like this – but it kinda makes me sweat. How do you know what is real? What if those objects were created under some other conditions and you have since edited your script? How many packages do you have loaded? What are they? It stresses me out, in part because you are violating Rule 1 – you don’t necessarily have a good track record of how that object was produced. It could be something that you executed long ago and you have since changed your script. You want your source of truth12 to be the script. In other words, the list of specfic commands that take you from raw data to your results. Zombie objects in the Global Environment are not your friend.\n\n\n\n\n\n\nTip: Set up a blank slate in RStudio by default\n\n\n\nThere is an easy way to set up a blank slate as RStudio’s default behavior. Just execute usethis::use_blank_slate() in the console and it will ensure that the Global Options of RStudio are configured in such a way that you have a blank slate each time you open R13. Alternatively, you can manually adjust the Global Options as explained here.\n\n\nIt is best practice to start with a blank slate every time you open RStudio. This will force you to rely solely on the code in front of you. Rather than something that may or may not be what you remember it to be. This approach also has the added benefit of mimicking the environment of someone else sitting down at their own machine, trying to replicate your results.\n\n\n\n\n\n\nThere are ultimately only two sources of truth\n\n\n\n\nRaw data\nSource code\n\nYou should build every project with that in mind. You need to ensure that anyone can get from your raw data to your results14 using only your scripts!"
  },
  {
    "objectID": "repro.html#projects-in-rstudio",
    "href": "repro.html#projects-in-rstudio",
    "title": "3  Reproducibility",
    "section": "3.4 Projects in RStudio",
    "text": "3.4 Projects in RStudio\n\n\n\n\n\n\nProjects are your friend.\n\n\n\nLet’s create a new project together now!\n\n\nJenny Bryan, a developer at RStudio, has an impassioned blog post on why you should embrace a project-oriented workflow. You should probably read her post in full, because if you don’t listen, she is threatening to set your computer on fire 🔥. But seriously, you should read it.\nAs a way of quickly summarizing one of her points: you should make sure that your final product (i.e., your script) is completely free of things that are specific to your own personal habits. For example, do you have something similar to setwd(\"/Users/tsoleary/R/workshop_2023\") at the start of your script? Or in some other way, are you using absolute paths? If you do, then for a certainty if someone else wants to run your code, they will have to edit it to make sure they don’t immediately run into an error. This means that right off the bat, your code is not reproducibility-friendly. As a remedy, she suggests using projects and the here package discussed below.\n\nhere package\nThe here package is a great way to make sure that your code can be run easily on someone else’s machine. Jenny Bryan has another post dedicated specifically to the here package: read it here.\n\n\n# Load data\ndat &lt;- readr::read_csv(here::here(\"data/raw/counts.csv\"))\n\n\n\n\n\n\n\nhere::here\n\n\n\nAs you’ll notice above rather than load the here package with library(here) and then use the here() function, I use the package::function_name notation to call the here function without attaching the whole here package. The added bonus is that it is kinda fun to say Here, Here! 🍺\n\n\nI find the here package very useful for working with RMarkdown documents. By default, RMarkdown documents often use what ever directory that document is in as its root directory, so then all relative paths are in relation to where ever that RMarkdown document happens to be. But the here package allows you to continue to use the project root for your relative paths!\n\n\n\n\n\n\nProjects in RStudio allow for easy integration with Version Control! Check out the short Section 3.8 on Version Control."
  },
  {
    "objectID": "repro.html#directory-structure",
    "href": "repro.html#directory-structure",
    "title": "3  Reproducibility",
    "section": "3.5 Directory structure",
    "text": "3.5 Directory structure\nThere are probably thousands of ways you could structure your files in a project – but there are really only two ways of going about it. The first is ad hoc. You group up files into subdirectores as you go along, tailoring the structure into something that makes sense to you, or at least something that is workable. And the other way, is to establish a template directory structure and build off that.\nFor most of the time I have worked in R, I have used the ad hoc approach. And the best I can say for it is that it can get the job done. In my eyes, each of those projects are their own unique snowflake. But if you ask someone else to look at it, they may think a dungeon maze is more apt a metaphor. Which of these scripts should I run first? Wait, where is all your raw data? Where are your final figures? It is best if the directory structure answers these questions on its own. So I have come to embrace a consistent directory structure.\n\n\n\n\n\n\nExample directory structures\n\n\n\nTake a look at my template directory structure.. I did a bunch of poking around on the internet and thinking about it and this is where I landed. It has already evolved somewhat since I started15. But here are a few useful links that I found in my stumblings that you should check out:\n\nPLOS Comp Bio: A quick guide to organizing computational biology projects (Noble 2009)\nBlog post with a few thoughts on strucuring R projects\nThe Johns Hopkins Data Science Lab tips for organizing projects.\nYoutube playlist by Danielle Navarro, author of the Learning Statistics with R.\nJenny Bryan’s thoughts on formatting from her course on stats\nData management plan that will make you laugh\n\n\n\nBelow is an example of a directory structure template:\n\nTemplateExample\n\n\n├── src/\n│   ├── 01_analysis/\n│   ├── 02_analysis/\n│   └── 03_figures/\n├── data/\n│   ├── raw/\n│   └── processed/\n├── output/\n│   ├── figs/\n│   └── tables/\n├── docs/\n│   └── index.qmd\n├── scratch/\n├── README.md\n└── .gitignore\n\nsrc – the source code, the ultimate source of truth\ndata – the raw data and intermediate processed data\noutput – output results and figures\ndocs – a place where I compile all results to share with collaborators\nscratch – messy code I have rec\n\n\n\nThis is a simplified example of the project I am working on right now.\n├── src/\n│   ├── 00_pheno/\n│   │   └── 00_pheno.R\n│   ├── 01_nuclei/\n│   │   ├── 00_count_nuclei.ijm\n│   ├── 02_cellranger-arc/\n│   │   ├── 00_mkref.sh\n│   │   ├── 01_count.sh\n│   │   └── 02_aggr.sh\n│   ├── 03_seurat/\n│   │   ├── 00_create_seurat_object.R\n│   │   ├── 01_quality_control_filtering.R\n│   │   └── 02_initial_cluster.R\n│   └── 04_plots/\n│   │   ├── annot.R\n│   │   ├── cluster.R\n│   │   └── final.R\n├── data/\n│   ├── raw/\n│   │   ├── annot/\n|   |   │   ├── calderon_markers.csv\n|   |   │   ├── dmel_cell-cycle_genes.csv\n|   |   │   └── insitu_annot.csv\n│   │   ├── nuclei/\n│   │   ├── pheno/\n│   │   └── seq/\n│   ├── processed/\n|   |   │   ├── annot.rds\n|   |   │   ├── cluster_all.rds\n|   |   │   └── cluster_manual.rds\n│   │   ├── annot/\n│   │   ├── genes/\n│   │   ├── seq/\n│   │   └── seurat_object/\n|   |   │   ├── 00_dat_raw.rds\n|   |   │   ├── 01_dat_qc.rds\n|   |   │   └── 02_dat_clust.rds\n├── docs/\n│   └── index.qmd\n├── output/\n│   ├── figs/\n|   │   ├── annot/\n|   |   │   ├── umap.pdf\n|   |   │   └── tsne.pdf\n|   │   ├── cluster/\n|   |   │   ├── umap.pdf\n|   |   │   └── tsne.pdf\n|   │   ├── final/\n|   |   │   ├── fig_1.pdf\n|   |   │   ├── fig_2.pdf\n|   |   │   └── fig_3.pdf\n│   ├── tables/\n│   ├── dars/\n|   │   ├── cell_type.rds\n|   │   └── cluster.rds\n│   └── degs/\n|   │   ├── cell_type.rds\n|   │   └── cluster.rds\n├── scratch/\n├── README.md\n└── .gitignore\n\n\n\n\n\n\n\n\n\nVersion control tip\n\n\n\nIf you use Version Control this sort of directory structure is also helpful because you can easily mark entire directories to be ignored (e.g., including data/* our output/figs/* in the .gitignore file). Most version control software will have a file size limit. Anyway, the thing you are most concerned with version controlling is the code (i.e., the src/ directory). The data and output can and should be backed up somewhere else."
  },
  {
    "objectID": "repro.html#scripts",
    "href": "repro.html#scripts",
    "title": "3  Reproducibility",
    "section": "3.6 Scripts",
    "text": "3.6 Scripts\nIf you read Jenny Bryan’s blog post on a project-oriented workflow referenced earlier, you likely ran across this advice:\n\nWhat about objects that take a long time to create? Isolate that bit in its own script and write the precious object to file with saveRDS(my_precious, here(\"results\", \"my_precious.rds\")). Now you can develop scripts to do downstream work that reload the precious object via my_precious &lt;- readRDS(here(\"results\", \"my_precious.rds\")). It is a good idea to break data analysis into logical, isolated pieces anyway.\n\nThis is a great way to code!\nImagine you have an RNA-sequencing project. You could create a massive script that does everything from data cleaning, to normalization, model selection, and figure creation. But because you don’t have things broken up into meaningful intermediate pieces\nThe journal will require you to provide the raw sequence files. But you should also provide your audience with the raw counts file, as well as a .rds file that contains the full DESeq2 object. This allows them to easily explore the data for themselves, rather than start from step zero. Then they could easily begin again at the model analysis step, without having to repeat the mapping steps.\n\n\n\n\n\n\nSplit up code into meaningful bite sized chunks\n\n\n\nFavor short scripts that do one thing, rather then a huge unruly script that does everything. This helps adhere to Rule 5: Record all intermediate results, when possible in standardized formats & Rule 8: Generate hierarchical analysis output, allowing layers of increasing detail to be inspected. Creating small scripts with intermediate results mean that you, in the future, or a reviewer can easily jump into the analysis mid-way and explore the data and repeat some analyses.\n\n\n\n\n\n\n\n\nAvoid magic numbers\n\n\n\nIt is a good idea to give numbers in your code a variable name. This makes it easier to know what that random number means.\nFor example, in the above code.\n\n\n\n3.6.1 Driver scripts\nIf you now have a bunch of small scripts that do small tasks, it can be useful to create a top-level script that executes all the code in the project, generating all plots and results. This both ensures that your results are reproducible and that if you need to change one small thing, like your input data, you can easily regenerate all your results.\n\n\n\nsrc/driver.R\n\n# ------------------------------------------------------------------------------\n# Simple example driver script to execute all scripts\n# TS O'Leary\n# ------------------------------------------------------------------------------\n\n# Source all files\nsource(here::here(\"src/heights/00_normalize.R\"))\nsource(here::here(\"src/analysis_1/01_analyze.R\"))\nsource(here::here(\"src/analysis_1/02_model.R\"))\nsource(here::here(\"src/analysis_1/03_integrate.R\"))\n\n\n\nUsing Rmarkdown as a driver script\nOne thing that I have been experimenting with is using Rmarkdown/Quarto as the driver script that both summarizes the results and can reproducibly run all the code.\n---\ntitle: \"How embryos acclimate to temperature through epigenetic regulation\"\nauthors: \"Thomas O’Leary\"\nformat:\n  html:\n    theme: lumen\n---\n  \n```{r init, filename = \"00_init_data.R\"}\n#| eval: false\n#| cache: true\n#| echo: true\n#| file: \"../src/00_pheno/00_init_data.R\"\n```\n\n```{r init, filename = \"01_norm_data.R\"}\n#| eval: false\n#| cache: true\n#| echo: true\n#| file: \"../src/00_pheno/01_norm_data.R\"\n```\nknitr cache time consuming chunks"
  },
  {
    "objectID": "repro.html#sec-vc",
    "href": "repro.html#sec-vc",
    "title": "3  Reproducibility",
    "section": "3.8 Version control",
    "text": "3.8 Version control\nThe details of the software are beyond the scope of this workshop, but one important way people ensure the reproducibility of their projects is to utilize Version Control. In short, Version Control is a useful way to make sure that as you edit and add to large projects over time you don’t lose or change any of the bits that make it work. For example, if you were to accidentally break a script, you could restore to a previous working version of that file, and then begin again. There are several software designed to do this, but the most popular in the data science world is Git and GitHub.\nThese tools can seem intimidating at first – especially because they typically are interfaced with in the command line. But if it makes you more comfortable, you can use the point-and-click approach to git within RStudio itself or a desktop clients (e.g., GitHub Desktop). Here is a very useful tutorial on how to use Git and GitHub within RStudio."
  },
  {
    "objectID": "repro.html#footnotes",
    "href": "repro.html#footnotes",
    "title": "3  Reproducibility",
    "section": "",
    "text": "Please see Goodman, Fanelli, and Ioannidis (2016) for a more rigorous discussion of terms.↩︎\nI hope the workshop helps 😁!↩︎\nI definitely don’t have a perfect workflow, although it has gotten better slowly over the years.↩︎\nIn particular, taking control of the small details that make your code easier to share and easier for others to understand↩︎\nGoogle data provenance and look at the flow charts.↩︎\nStatistical programming is right in R’s wheelhouse. And it is also the most common type of programming for early career students in my discipline. You design and conduct an experiment. You generate data. You analyze data. You present data.↩︎\nAs with all advice in these workshop, these are just my opinions – no more. And as with all rules, there are always good exceptions to breaking any of these rules.↩︎\nIf you find yourself making a bunch of files with the same prefix, that probably means that they should all be in their own directory.↩︎\nIf in the future, you happen to have a new machine that doesn’t have some_package installed, you will remember how to install it. This is something that can just be run directly in the console, when necessary, and does not need to be saved in the script.↩︎\nIf you want more context to some of these tips, please read on!↩︎\nEither in reality or just as a mental exercise.↩︎\nRead this link to R for Data Science, for more information.↩︎\nRemember: You may need to install the usethis package install.packages(\"usethis\").↩︎\nReproducibility in a nutshell.↩︎\nReproducubility is a process↩︎"
  },
  {
    "objectID": "tidyverse.html#why-use-the-the-tidyverse",
    "href": "tidyverse.html#why-use-the-the-tidyverse",
    "title": "1  Tidyverse",
    "section": "1.1 Why use the the tidyverse?",
    "text": "1.1 Why use the the tidyverse?\nThere are base R equivalents to most things that you can do in the tidyverse. So you might wonder why it is necessary, or even useful, to learn the tidyverse. I think that skepticism is fair, but here are a few reasons why I think you should not only learn the tidyverse, but make it a regular part of your workflow1.\n\nAs a data scientist, most of your time will be spent writing code, not waiting for code to execute. Therefore you should put more value the coding style that is easiest to write and understand. The tidyverse emphasizes human readable code.\nBecause of the flexible nature of some of the tidyverse functions, the tidyverse can make for more reproducible code, that is less likely to break if some of the underlying data has changed or been added too.\nAn increasing number of newly developed packages depend on the tidyverse. So you might as well embrace it.\nYou are already using it! If have made any figures in R, more likely than not, you are already used one tidyverse package, ggplot2.\n\n\n1.1.1 Tidyverse style\n\n\n\n\n\n\nThe tidy tools manifesto\n\n\n\nMy favorite part of the tidyverse is the final principle in this manifesto:\n\nDesign for humans. “Programs must be written for people to read, and only incidentally for machines to execute.” — Hal Abelson\n\n\n\nThe tidyverse has a style guide that gives advice on everything from file names, to syntax, spacing, and function names. Although following these rules isn’t at all necessary, it good coding practice to have a consistent coding style. It will make it easier for you to read and prevent you from making as many mistakes. This will also save you some time, because you will spend less time making those small decisions if you fully adopt a specific coding style. We will check out some tools for automating your style later at the end of the workshop!\n\n“Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread.” 2\n\n\n\n\n\n\n\nCheat sheets\n\n\n\nTidyverse packages can help with many common tasks. Check out these cheat sheets for quick reference.\n\nData import\nData tidying\nData wrangling\nData transformation\nData visualization\nFunctional programming\nStrings\nFactors\n\n\n\n\n# Load library\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the \u001b]8;;http://conflicted.r-lib.org/\u0007conflicted package\u001b]8;;\u0007 to force all conflicts to become errors\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\n\n\nRather than loading each package individually – I often just load all of the core packages with the convenient library(tidyverse)3. Most of my scripts are dependent on at least three of the core packages, so it is easier to just have them all included. But you could load each individually."
  },
  {
    "objectID": "tidyverse.html#tidy-data",
    "href": "tidyverse.html#tidy-data",
    "title": "1  Tidyverse",
    "section": "1.2 Tidy data",
    "text": "1.2 Tidy data\nThe tidyverse gets its name from the type of data that it is designed to interact with – tidy data. So let’s quickly define tidy data4.\n\n\n\n\n\n\nTidy data – definition\n\n\n\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\n\n\nBut that definition is a bit abstract – especially if you have never thought about whether data is messy or tidy before – so let’s try a practical example.\n\n\n\n\n\n\nImagine over the course of several weeks, you measure the height of a few plants. If you record the data by hand or in a spread sheet. It is likely to look something like the below data.\n\n\n\n\n\nCode\n# Number of plants\nn &lt;- 10\n\n# Make up some random data\ndat &lt;- tibble(\n  plant_id = 1:n,\n  treatment = c(rep(\"control\", n/2), rep(\"treatment\", n/2)),\n  week_1 = rnorm(n, mean = 10, sd = 3),\n  week_2 = week_1*runif(n, min = 1, max = 1.5),\n  week_3 = week_2*runif(n, min = 1, max = 1.5),\n  week_4 = week_3*runif(n, min = 1, max = 1.5),\n  week_5 = week_4*runif(n, min = 1, max = 1.5)\n  ) |&gt; \n  mutate_if(is.numeric, round, 1)\n\n# Print out pretty table\ndat |&gt; \n  kableExtra::kable(\"html\") |&gt; \n  kableExtra::kable_styling(\"striped\")\n\n\n\n\n\nplant_id\ntreatment\nweek_1\nweek_2\nweek_3\nweek_4\nweek_5\n\n\n\n\n1\ncontrol\n4.0\n4.9\n6.0\n8.5\n11.6\n\n\n2\ncontrol\n10.9\n12.7\n15.2\n20.2\n20.8\n\n\n3\ncontrol\n10.2\n11.9\n13.3\n14.6\n21.6\n\n\n4\ncontrol\n11.2\n12.9\n13.6\n20.1\n28.2\n\n\n5\ncontrol\n11.0\n16.2\n21.1\n27.7\n35.4\n\n\n6\ntreatment\n10.8\n12.7\n16.7\n19.1\n28.2\n\n\n7\ntreatment\n10.0\n12.4\n16.5\n21.3\n21.4\n\n\n8\ntreatment\n14.8\n17.2\n24.4\n32.1\n33.9\n\n\n9\ntreatment\n11.9\n16.9\n21.6\n30.4\n42.5\n\n\n10\ntreatment\n8.6\n9.2\n10.6\n12.4\n13.2\n\n\n\n\n\n\n\nThe above is messy data. The week variable is hidden in the column names and for each row there are as many observations as there are weeks (i.e., in this case there are five observations in each row). This way is probably how you naturally record data.\nBut in reality, it is better\n\n\nCode\n# Pivot data to tidy format and then convert week to numeric\ndat_long &lt;- dat |&gt;\n  pivot_longer(contains(\"week\"),\n               names_to = \"week\",\n               values_to = \"height_cm\") |&gt; \n  mutate(week = as.numeric(str_remove_all(week, \"week_\")))\n\n# Print out pretty table\ndat_long |&gt; \n  kableExtra::kable(\"html\") |&gt; \n  kableExtra::kable_styling(\"striped\") |&gt; \n  kableExtra::scroll_box(height = \"500px\")\n\n\n\n\n\n\nplant_id\ntreatment\nweek\nheight_cm\n\n\n\n\n1\ncontrol\n1\n4.0\n\n\n1\ncontrol\n2\n4.9\n\n\n1\ncontrol\n3\n6.0\n\n\n1\ncontrol\n4\n8.5\n\n\n1\ncontrol\n5\n11.6\n\n\n2\ncontrol\n1\n10.9\n\n\n2\ncontrol\n2\n12.7\n\n\n2\ncontrol\n3\n15.2\n\n\n2\ncontrol\n4\n20.2\n\n\n2\ncontrol\n5\n20.8\n\n\n3\ncontrol\n1\n10.2\n\n\n3\ncontrol\n2\n11.9\n\n\n3\ncontrol\n3\n13.3\n\n\n3\ncontrol\n4\n14.6\n\n\n3\ncontrol\n5\n21.6\n\n\n4\ncontrol\n1\n11.2\n\n\n4\ncontrol\n2\n12.9\n\n\n4\ncontrol\n3\n13.6\n\n\n4\ncontrol\n4\n20.1\n\n\n4\ncontrol\n5\n28.2\n\n\n5\ncontrol\n1\n11.0\n\n\n5\ncontrol\n2\n16.2\n\n\n5\ncontrol\n3\n21.1\n\n\n5\ncontrol\n4\n27.7\n\n\n5\ncontrol\n5\n35.4\n\n\n6\ntreatment\n1\n10.8\n\n\n6\ntreatment\n2\n12.7\n\n\n6\ntreatment\n3\n16.7\n\n\n6\ntreatment\n4\n19.1\n\n\n6\ntreatment\n5\n28.2\n\n\n7\ntreatment\n1\n10.0\n\n\n7\ntreatment\n2\n12.4\n\n\n7\ntreatment\n3\n16.5\n\n\n7\ntreatment\n4\n21.3\n\n\n7\ntreatment\n5\n21.4\n\n\n8\ntreatment\n1\n14.8\n\n\n8\ntreatment\n2\n17.2\n\n\n8\ntreatment\n3\n24.4\n\n\n8\ntreatment\n4\n32.1\n\n\n8\ntreatment\n5\n33.9\n\n\n9\ntreatment\n1\n11.9\n\n\n9\ntreatment\n2\n16.9\n\n\n9\ntreatment\n3\n21.6\n\n\n9\ntreatment\n4\n30.4\n\n\n9\ntreatment\n5\n42.5\n\n\n10\ntreatment\n1\n8.6\n\n\n10\ntreatment\n2\n9.2\n\n\n10\ntreatment\n3\n10.6\n\n\n10\ntreatment\n4\n12.4\n\n\n10\ntreatment\n5\n13.2\n\n\n\n\n\n\n\n\n\n\n\n\n\nBAM! That is tidy data.\n\n\n\n\nEvery column is a variable – plant_id, treatment, time, height.\nEvery row is an observation – a height measurement on a single plant.\nEvery cell is a single value.\n\nFor right now, no need to worry about the code used to convert the original data to tidy format. Although if you are curious, take a look under the hood. We will return to this later in the workshop."
  },
  {
    "objectID": "tidyverse.html#advanced-techniques",
    "href": "tidyverse.html#advanced-techniques",
    "title": "1  Tidyverse",
    "section": "1.4 Advanced techniques",
    "text": "1.4 Advanced techniques\n\nOther useful tidyverse packages\n\nbroom – clean model output – technically a core package within tidymodels\ntidymodels – https://www.tidymodels.org/packages/\nrvest – web scraping – mining data from a website\nmodelr – modelling – support for modelling data in the tidyverse"
  },
  {
    "objectID": "tidyverse.html#importing-data",
    "href": "tidyverse.html#importing-data",
    "title": "1  Tidyverse",
    "section": "1.7 Importing data",
    "text": "1.7 Importing data\nI find it a lot easier to define the data structure as you import data. And it creates a tibble instead of data.frame.\nThere are several reasons to prefer readr::read_csv() over base R’s read.csv.\n\n# Create a .csv file to import\nwrite_csv(iris, \"iris.csv\")\n\n\n# Try read.csv\nd.f &lt;- read.csv(\"iris.csv\")\n\nstr(d.f)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : chr  \"setosa\" \"setosa\" \"setosa\" \"setosa\" ...\n\n\n\n# Try read_csv\nread_csv(\"iris.csv\")\n\nRows: 150 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Species\ndbl (4): Sepal.Length, Sepal.Width, Petal.Length, Petal.Width\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 140 more rows\n\n# Set col_type as you import data -- allows you to define level order too\nd_f &lt;- read_csv(\"iris.csv\",\n                col_types = list(Species = col_factor(c(\"versicolor\",\n                                                        \"setosa\", \n                                                        \"virginica\"))))\nd_f\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 140 more rows\n\nstr(d_f)\n\nspc_tbl_ [150 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Sepal.Length: num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"versicolor\",\"setosa\",..: 2 2 2 2 2 2 2 2 2 2 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Sepal.Length = col_double(),\n  ..   Sepal.Width = col_double(),\n  ..   Petal.Length = col_double(),\n  ..   Petal.Width = col_double(),\n  ..   Species = col_factor(levels = c(\"versicolor\", \"setosa\", \"virginica\"), ordered = FALSE, include_na = FALSE)\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nglimpse(d_f)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\n\n1.7.1 nest\nFor the most part, I find myself working with 2D structured data (i.e., table with rows and columns). But sometimes, it can help to you need to\nLet’s use the\n\ndat &lt;- ChickWeight |&gt; \n  janitor::clean_names() |&gt; \n  as_tibble()\n\ndat |&gt;\n  glimpse()\n\nRows: 578\nColumns: 4\n$ weight &lt;dbl&gt; 42, 51, 59, 64, 76, 93, 106, 125, 149, 171, 199, 205, 40, 49, 5…\n$ time   &lt;dbl&gt; 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 21, 0, 2, 4, 6, 8, 10, 1…\n$ chick  &lt;ord&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ diet   &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\n\n\ndat |&gt;\n  group_by(chick, diet) |&gt;\n  nest() \n\n# A tibble: 50 × 3\n# Groups:   chick, diet [50]\n   chick diet  data             \n   &lt;ord&gt; &lt;fct&gt; &lt;list&gt;           \n 1 1     1     &lt;tibble [12 × 2]&gt;\n 2 2     1     &lt;tibble [12 × 2]&gt;\n 3 3     1     &lt;tibble [12 × 2]&gt;\n 4 4     1     &lt;tibble [12 × 2]&gt;\n 5 5     1     &lt;tibble [12 × 2]&gt;\n 6 6     1     &lt;tibble [12 × 2]&gt;\n 7 7     1     &lt;tibble [12 × 2]&gt;\n 8 8     1     &lt;tibble [11 × 2]&gt;\n 9 9     1     &lt;tibble [12 × 2]&gt;\n10 10    1     &lt;tibble [12 × 2]&gt;\n# … with 40 more rows\n\n\n\ndat_nest &lt;- dat |&gt;\n  group_by(chick, diet) |&gt;\n  nest() \n\ndat_nest\n\n# A tibble: 50 × 3\n# Groups:   chick, diet [50]\n   chick diet  data             \n   &lt;ord&gt; &lt;fct&gt; &lt;list&gt;           \n 1 1     1     &lt;tibble [12 × 2]&gt;\n 2 2     1     &lt;tibble [12 × 2]&gt;\n 3 3     1     &lt;tibble [12 × 2]&gt;\n 4 4     1     &lt;tibble [12 × 2]&gt;\n 5 5     1     &lt;tibble [12 × 2]&gt;\n 6 6     1     &lt;tibble [12 × 2]&gt;\n 7 7     1     &lt;tibble [12 × 2]&gt;\n 8 8     1     &lt;tibble [11 × 2]&gt;\n 9 9     1     &lt;tibble [12 × 2]&gt;\n10 10    1     &lt;tibble [12 × 2]&gt;\n# … with 40 more rows\n\ndat_nest$data[1]\n\n[[1]]\n# A tibble: 12 × 2\n   weight  time\n    &lt;dbl&gt; &lt;dbl&gt;\n 1     42     0\n 2     51     2\n 3     59     4\n 4     64     6\n 5     76     8\n 6     93    10\n 7    106    12\n 8    125    14\n 9    149    16\n10    171    18\n11    199    20\n12    205    21\n\n\n\n\n1.7.2 broom\n\n# Load library\nlibrary(broom)\n\nCheck out the broom vignette.\nAnd the broom and dplyr vignette.\ntidy: constructs a tibble that summarizes the model’s statistical findings. This includes coefficients and p-values for each term in a regression, per-cluster information in clustering applications, or per-test information for multtest functions.\nglance: construct a concise one-row summary of the model. This typically contains values such as R^2, adjusted R^2, and residual standard error that are computed once for the entire model.\n\nlinear_model_results &lt;- dat |&gt;\n  group_by(chick, diet) |&gt;\n  nest() |&gt;\n  mutate(\n    fit = map(data, ~ lm(weight ~ time, data = .x)),\n    tidied = map(fit, tidy),\n    glanced = map(fit, glance)\n  ) |&gt; \n  unnest(tidied) \n\nlinear_model_results\n\n# A tibble: 100 × 10\n# Groups:   chick, diet [50]\n   chick diet  data     fit    term    estim…¹ std.e…² stati…³  p.value glanced \n   &lt;ord&gt; &lt;fct&gt; &lt;list&gt;   &lt;list&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;list&gt;  \n 1 1     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   24.5    6.73     3.64 4.56e- 3 &lt;tibble&gt;\n 2 1     1     &lt;tibble&gt; &lt;lm&gt;   time       7.99   0.524   15.3  2.97e- 8 &lt;tibble&gt;\n 3 2     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   24.7    4.93     5.01 5.26e- 4 &lt;tibble&gt;\n 4 2     1     &lt;tibble&gt; &lt;lm&gt;   time       8.72   0.384   22.7  6.15e-10 &lt;tibble&gt;\n 5 3     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   23.2    5.08     4.56 1.04e- 3 &lt;tibble&gt;\n 6 3     1     &lt;tibble&gt; &lt;lm&gt;   time       8.49   0.396   21.5  1.08e- 9 &lt;tibble&gt;\n 7 4     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   32.9    4.01     8.21 9.42e- 6 &lt;tibble&gt;\n 8 4     1     &lt;tibble&gt; &lt;lm&gt;   time       6.09   0.312   19.5  2.70e- 9 &lt;tibble&gt;\n 9 5     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   16.9    7.56     2.24 4.93e- 2 &lt;tibble&gt;\n10 5     1     &lt;tibble&gt; &lt;lm&gt;   time      10.1    0.588   17.1  9.88e- 9 &lt;tibble&gt;\n# … with 90 more rows, and abbreviated variable names ¹​estimate, ²​std.error,\n#   ³​statistic\n\n\n\nlinear_model_results |&gt; \n  filter(term == \"time\") |&gt; \n  group_by(diet) |&gt; \n  summarize(avg_slope = mean(estimate))\n\n# A tibble: 4 × 2\n  diet  avg_slope\n  &lt;fct&gt;     &lt;dbl&gt;\n1 1          5.85\n2 2          8.61\n3 3         11.4 \n4 4          9.52\n\n\n\nChickWeight |&gt;\n  ggplot() +\n  geom_line(aes(x = Time, \n                 y = weight, \n                 color = Chick)) +\n  facet_wrap(~ Diet)\n\n\n\n\n\n\n\n\nWickham, Hadley. 2014. “Tidy Data.” J. Stat. Softw. 59 (September): 1–23."
  },
  {
    "objectID": "tidyverse.html#tools-for-quick-clean-up",
    "href": "tidyverse.html#tools-for-quick-clean-up",
    "title": "1  Tidyverse",
    "section": "1.6 Tools for quick clean up",
    "text": "1.6 Tools for quick clean up\n\n\n\n\n\n\nCaution\n\n\n\nA hazard of caring about how coding style is that you will notice bad formatting everywhere. But there are quick tools too help format messy code and data.\n\n\n\n1.6.1 Styler package\nThe styler package is an Addin within RStudio that can format messy code into a custom coding style. Check it out!\n\n\n1.6.2 Janitor package\nThe janitor package makes cleaning up messy data names a lot easier. For example, if you are like me, you had some trouble remembering which letters were capitalized in the ChickWeight data (i.e., the independent variables, Chick, Diet, and Time are capitalized but the dependent variable weight isn’t). I find this super annoying, but with the janitor package\nInstall with install.packages(\"janitor\") and check it out!\n\niris |&gt; \n  colnames()\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\nChickWeight |&gt; \n  colnames()\n\n[1] \"weight\" \"Time\"   \"Chick\"  \"Diet\"  \n\n\n\niris |&gt; \n  janitor::clean_names() |&gt; \n  colnames()\n\n[1] \"sepal_length\" \"sepal_width\"  \"petal_length\" \"petal_width\"  \"species\"     \n\nChickWeight |&gt; \n  janitor::clean_names() |&gt; \n  colnames()\n\n[1] \"weight\" \"time\"   \"chick\"  \"diet\""
  },
  {
    "objectID": "tidyverse.html#wrangling-data",
    "href": "tidyverse.html#wrangling-data",
    "title": "1  Tidyverse",
    "section": "1.8 Wrangling data",
    "text": "1.8 Wrangling data\n\n1.8.1 Intro to dplyr\ndplyr has a few core functions that are built to work on\nTaken from the dplyr vignette.\n\n1.8.1.1 Rows\n\nfilter() chooses rows based on column values.\nslice() chooses rows based on location.\narrange() changes the order of the rows.\n\n\n\n1.8.1.2 Columns\n\nselect() changes whether or not a column is included.\nrename() changes the name of columns.\nmutate() changes the values of columns and creates new columns.\nrelocate() changes the order of the columns.\n\n\n\n1.8.1.3 Groups of rows\n\nsummarise() collapses a group into a single row.\n\n\n\n\n1.8.2 Pipe %&gt;%\nFrom magrittr.\n\nx %&gt;% f is equivalent to f(x)\nx %&gt;% f(y) is equivalent to f(x, y)\nx %&gt;% f %&gt;% g %&gt;% h is equivalent to h(g(f(x)))\n\nThe argument placeholder\n\nx %&gt;% f(y, .) is equivalent to f(y, x)\nx %&gt;% f(y, z = .) is equivalent to f(y, z = x)\n\n\n\n\n\n\n\n|&gt; native pipe operator\n\n\n\nIn new versions of R (&gt; 4.1.0), there is now a native pipe operator that comes within the syntax of R itself, rather than being loaded as a part of a separate package.\nAlthough they are largely the same, there are some meaningful differences between the two. These two articles from the tidyverse and Isabella Velásquez do a good job of explaining the differences relevant differences.\nI have started just using the native pipe (|&gt;) for almost all cases now. First, becuase it just looks cleaner. But also because then you can use a pipe without explicitly loading the mattingr package (_e.g. iris |&gt; dplyr::glimpse()). Where if you had used the magrittr pipe (%&gt;%), but not loaded the magrittr package, you would get an error.\n\n\n\n1.8.2.1 A fun example from R for data science\n\nfoo_foo &lt;- little_bunny()\nfoo_foo_1 &lt;- hop(foo_foo, through = forest)\nfoo_foo_2 &lt;- scoop(foo_foo_1, up = field_mice)\nfoo_foo_3 &lt;- bop(foo_foo_2, on = head)\n\n\nfoo_foo |&gt;\n  hop(through = forest) |&gt;\n  scoop(up = field_mice) |&gt;\n  bop(on = head)\n\n\n\n\n1.8.3 arrange\n\n# dat |&gt;\n#   arrange(tissue, iu_gfw)\n\n\n# dat |&gt;\n#   arrange(desc(tissue), desc(iu_gfw))\n\n\n\n1.8.4 summarise\n\n# dat |&gt;\n#   summarise(iu_gfw_avg = mean(iu_gfw))\n\n\n\n1.8.5 group_by\n\n# dat |&gt;\n#   group_by(tissue) |&gt;\n#   summarise(iu_gfw_avg = mean(iu_gfw),\n#             iu_gfw_sd = sd(iu_gfw))\n\nWarning that you must be careful about the order when reusing variable names.\n\n# # Bad order\n# dat |&gt;\n#   group_by(tissue) |&gt;\n#   summarise(iu_gfw = mean(iu_gfw),\n#             sd = sd(iu_gfw))\n\n\n# # This order works because it collapses the data into a mean last\n# dat |&gt;\n#   group_by(tissue) |&gt;\n#   summarise(sd = sd(iu_gfw),\n#             iu_gfw = mean(iu_gfw))\n\nPrint out a pretty table using kableExtra.\n\n\n1.8.6 nest\nFor the most part, I find myself working with 2D structured data (e.g., tibble or data.frame). But sometimes you need to\n\nChickWeight |&gt;\n  glimpse()\n\nRows: 578\nColumns: 4\n$ weight &lt;dbl&gt; 42, 51, 59, 64, 76, 93, 106, 125, 149, 171, 199, 205, 40, 49, 5…\n$ Time   &lt;dbl&gt; 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 21, 0, 2, 4, 6, 8, 10, 1…\n$ Chick  &lt;ord&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ Diet   &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\n\n\nChickWeight |&gt;\n  group_by(Chick, Diet) |&gt;\n  nest() \n\n# A tibble: 50 × 3\n# Groups:   Chick, Diet [50]\n   Chick Diet  data             \n   &lt;ord&gt; &lt;fct&gt; &lt;list&gt;           \n 1 1     1     &lt;tibble [12 × 2]&gt;\n 2 2     1     &lt;tibble [12 × 2]&gt;\n 3 3     1     &lt;tibble [12 × 2]&gt;\n 4 4     1     &lt;tibble [12 × 2]&gt;\n 5 5     1     &lt;tibble [12 × 2]&gt;\n 6 6     1     &lt;tibble [12 × 2]&gt;\n 7 7     1     &lt;tibble [12 × 2]&gt;\n 8 8     1     &lt;tibble [11 × 2]&gt;\n 9 9     1     &lt;tibble [12 × 2]&gt;\n10 10    1     &lt;tibble [12 × 2]&gt;\n# … with 40 more rows\n\n\n\nChickWeight_nest &lt;- ChickWeight |&gt;\n  group_by(Chick, Diet) |&gt;\n  nest() \n\nChickWeight_nest$data[1:2]\n\n[[1]]\n# A tibble: 12 × 2\n   weight  Time\n    &lt;dbl&gt; &lt;dbl&gt;\n 1     42     0\n 2     51     2\n 3     59     4\n 4     64     6\n 5     76     8\n 6     93    10\n 7    106    12\n 8    125    14\n 9    149    16\n10    171    18\n11    199    20\n12    205    21\n\n[[2]]\n# A tibble: 12 × 2\n   weight  Time\n    &lt;dbl&gt; &lt;dbl&gt;\n 1     40     0\n 2     49     2\n 3     58     4\n 4     72     6\n 5     84     8\n 6    103    10\n 7    122    12\n 8    138    14\n 9    162    16\n10    187    18\n11    209    20\n12    215    21\n\n\n\n\n1.8.7 broom\n\n# Load library\nlibrary(broom)\n\nCheck out the broom vignette.\nAnd the broom and dplyr vignette.\ntidy: constructs a tibble that summarizes the model’s statistical findings. This includes coefficients and p-values for each term in a regression, per-cluster information in clustering applications, or per-test information for multtest functions.\nglance: construct a concise one-row summary of the model. This typically contains values such as R^2, adjusted R^2, and residual standard error that are computed once for the entire model.\n\nChickWeight |&gt;\n  group_by(Chick, Diet) |&gt;\n  nest() |&gt;\n  mutate(\n    fit = map(data, ~ lm(weight ~ Time, data = .x)),\n    tidied = map(fit, tidy),\n    glanced = map(fit, glance)\n  ) |&gt; \n  unnest(tidied) \n\n# A tibble: 100 × 10\n# Groups:   Chick, Diet [50]\n   Chick Diet  data     fit    term    estim…¹ std.e…² stati…³  p.value glanced \n   &lt;ord&gt; &lt;fct&gt; &lt;list&gt;   &lt;list&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;list&gt;  \n 1 1     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   24.5    6.73     3.64 4.56e- 3 &lt;tibble&gt;\n 2 1     1     &lt;tibble&gt; &lt;lm&gt;   Time       7.99   0.524   15.3  2.97e- 8 &lt;tibble&gt;\n 3 2     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   24.7    4.93     5.01 5.26e- 4 &lt;tibble&gt;\n 4 2     1     &lt;tibble&gt; &lt;lm&gt;   Time       8.72   0.384   22.7  6.15e-10 &lt;tibble&gt;\n 5 3     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   23.2    5.08     4.56 1.04e- 3 &lt;tibble&gt;\n 6 3     1     &lt;tibble&gt; &lt;lm&gt;   Time       8.49   0.396   21.5  1.08e- 9 &lt;tibble&gt;\n 7 4     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   32.9    4.01     8.21 9.42e- 6 &lt;tibble&gt;\n 8 4     1     &lt;tibble&gt; &lt;lm&gt;   Time       6.09   0.312   19.5  2.70e- 9 &lt;tibble&gt;\n 9 5     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   16.9    7.56     2.24 4.93e- 2 &lt;tibble&gt;\n10 5     1     &lt;tibble&gt; &lt;lm&gt;   Time      10.1    0.588   17.1  9.88e- 9 &lt;tibble&gt;\n# … with 90 more rows, and abbreviated variable names ¹​estimate, ²​std.error,\n#   ³​statistic\n\n\n\nChickWeight |&gt;\n  ggplot() +\n  geom_line(aes(x = Time, \n                 y = weight, \n                 color = Chick)) +\n  facet_wrap(~ Diet)\n\n\n\n\n\n\n\n\nWickham, Hadley. 2014. “Tidy Data.” J. Stat. Softw. 59 (September): 1–23."
  },
  {
    "objectID": "tidyverse.html#footnotes",
    "href": "tidyverse.html#footnotes",
    "title": "1  Tidyverse",
    "section": "",
    "text": "Maybe even convert some of your old scripts to incorporate the tidyverse.↩︎\nHonestly I think that joke underestimates how important good coding style is. You can actually read “butitsuremakesthingseasiertoread” pretty easily because you are an expert reader – you’ve been at it everyday for decades – coding, probably not so much. I don’t think can overstate how important I think it is to write visually pleasing code.↩︎\nNotice above that this function call loads nine core tidyverse packages. Any other tidyverse packages, including some we will cover later (e.g., broom), must be loaded separately.↩︎\nWickham (2014)↩︎\nThis argument placeholder is the biggest difference between the magrittr pipe %&gt;% and the native pipe |&gt;.↩︎\nAlthough, this data happens to be close to linear.↩︎\nWe will assume that it is linear↩︎\nreadr::write_csv() doesn’t write the rownames to the file, which is wonderful.↩︎"
  },
  {
    "objectID": "data_viz.html#gallery-of-visualizations",
    "href": "data_viz.html#gallery-of-visualizations",
    "title": "2  Data visualization",
    "section": "2.1 Gallery of visualizations",
    "text": "2.1 Gallery of visualizations\n\nOcean temperaturePoliticsCollege admissions\n\n\n\n\n\nhttps://www.nytimes.com/interactive/2023/08/03/climate/ocean-temperatures-heat-earth.html\n\n\n\n\n\nhttps://www.nytimes.com/interactive/2023/08/03/climate/ocean-temperatures-heat-earth.html\n\n\n\n\n\n\n\nhttps://www.nytimes.com/2023/08/01/us/politics/biden-trump-poll.html\n\n\n\n\n\n\n\nhttps://www.nytimes.com/interactive/2023/07/24/upshot/ivy-league-elite-college-admissions.html\n\n\n\n\n\nhttps://www.nytimes.com/interactive/2023/07/24/upshot/ivy-league-elite-college-admissions.html"
  },
  {
    "objectID": "data_viz.html#explore-the-data",
    "href": "data_viz.html#explore-the-data",
    "title": "2  Data visualization",
    "section": "2.2 Explore the data",
    "text": "2.2 Explore the data\n\n\n\n\n\n\nMeet the Palmer penguins\n\n\n\n\n\n\nArtwork by @allison_horst\n\n\nAn alternative to the iris data set.\n\n\n\n# Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the \u001b]8;;http://conflicted.r-lib.org/\u0007conflicted package\u001b]8;;\u0007 to force all conflicts to become errors\n\nlibrary(palmerpenguins)\n\n\n# Take a peak at the data\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…"
  },
  {
    "objectID": "data_viz.html#choosing-a-type-of-visualization",
    "href": "data_viz.html#choosing-a-type-of-visualization",
    "title": "2  Data visualization",
    "section": "2.3 Choosing a type of visualization",
    "text": "2.3 Choosing a type of visualization\nChoosing a type of visualization depends entirely on the sorts of questions you are trying to ask. So for now, let’s pick a specific question. I don’t know anything about these penguins. So here are some simple questions that I have brainstormed:\n\nAre different species of penguins different weights?\nWhat is the distribution of species on each island?\nDoes the correlation between flipper length and body mass vary between species?\n\nThe first question is probably the easiest to imagine visualizing so we will start with that. But it is worth trying the others.\n\n\n\n\n\n\nPractice: Let’s build these plots from the ground up\n\n\n\nI really hate the default theme for ggplot. I can’t even stand looking at it. We will get into some thoughts on themes later. But for now, let’s just set something that is a little easier on the eyes. This command below will set cowplot::theme_minimal_grid() as the default theme for all plots going forward.\n\n\n\n# Set a nicer looking theme as a place holder\ntheme_set(cowplot::theme_minimal_grid())\n\n\nBox plotViolin plotStrip plotBeeswarmHistogramDensity plot\n\n\n\npenguins |&gt; \n  ggplot() +\n  geom_boxplot(\n    aes(x = species,\n        y = body_mass_g))\n\n\n\n\n\n\n\npenguins |&gt; \n  ggplot() +\n  geom_violin(\n    aes(x = species,\n        y = body_mass_g))\n\n\n\n\n\n\n\npenguins |&gt; \n  ggplot() +\n  geom_jitter(\n    aes(x = species,\n        y = body_mass_g),\n    width = 0.1)\n\n\n\n\n\n\n\npenguins |&gt; \n  ggplot() +\n  ggbeeswarm::geom_beeswarm(\n    corral.width = 2.0,\n    aes(x = species,\n        y = body_mass_g)) \n\n\n\n\n\n\n\npenguins |&gt; \n  ggplot() +\n  geom_histogram(\n    aes(x = body_mass_g),\n    color = \"grey20\",\n    fill = \"grey80\",\n    bins = 30) +\n  facet_wrap(~species,\n             nrow = 3)\n\n\n\n\n\n\n\npenguins |&gt; \n  ggplot() +\n  geom_density(\n    aes(x = body_mass_g,\n        color = species,\n        fill = species),\n    alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice the advantages and disadvantages of the different styles.\n\n\n\nSome have more information than others. If you had to pick one? Which would you pick?\n\n\n\n\n\n\n\n\nFortunately we don’t have to pick just one.\nI think choosing the box plot and individual points is the best of both worlds, because you have both the individual data points and summary statistics. But\n\n\n\n:::{.panel-tabset}\n\n2.3.1 Box-and-dots\n\npenguins |&gt; \n  ggplot(\n    aes(x = species,\n        y = body_mass_g)\n    ) +\n  geom_boxplot(width = 0.3) +\n  geom_jitter(width = 0.1,\n              shape = 21)\n\n\n\n\n\n\n2.3.2 Box-and-swarm\n\npenguins |&gt; \n  ggplot(\n    aes(x = species,\n        y = body_mass_g)\n    ) +\n  geom_boxplot(width = 0.4) +\n  ggbeeswarm::geom_beeswarm(shape = 21,\n                            fill = \"grey90\",\n                            alpha = 0.7)\n\n\n\n\n\n\n2.3.3 Box-and-violin\n\npenguins |&gt; \n  ggplot(\n    aes(x = species,\n        y = body_mass_g)\n    ) +\n  geom_violin(width = 0.8) + \n  geom_boxplot(width = 0.2)"
  },
  {
    "objectID": "data_viz.html#limits",
    "href": "data_viz.html#limits",
    "title": "2  Data visualization",
    "section": "2.4 Limits",
    "text": "2.4 Limits\n\n\n\n\n\n\nIncluding zero on the axis\n\n\n\nYou should favor including zero on the axis when it is appropriate. For example, in the versions of the figure we have made so far, it looks like Gentoo is double the body mass of the other two species, but the difference is more subtle than that as you can see when you include zero.\nYou do not need to be a zero absolutist, but use your judgement for when it makes sense to include zero, and when it is misleading to omit zero. There are variables where zero has no meaning and therefore does not need to be included (e.g. temperature). And there are occasions where including zero can minimize otherwise important variation in your data (e.g. life expectancy). But there are ways around it. Check out this example.\n\n\n\n# Added zero on the vertical axis\npenguins |&gt; \n  ggplot(\n    aes(x = species,\n        y = body_mass_g)\n    ) +\n  geom_boxplot(width = 0.5) +\n  ggbeeswarm::geom_beeswarm(shape = 21,\n                            fill = \"grey90\",\n                            alpha = 0.7) +\n  scale_y_continuous(limits = c(0, 7000))\n\n\n\n\n\npenguins |&gt; \n  ggplot(\n    aes(x = species,\n        y = body_mass_g)\n    ) +\n  geom_violin(width = 0.8) + \n  geom_boxplot(width = 0.2) +\n  scale_y_continuous(\n    limits = c(0, 7000)\n  )"
  },
  {
    "objectID": "data_viz.html#creating-the-axes",
    "href": "data_viz.html#creating-the-axes",
    "title": "2  Data visualization",
    "section": "2.5 Creating the axes",
    "text": "2.5 Creating the axes\n\npenguins |&gt; \n  ggplot(\n    aes(x = species,\n        y = body_mass_g)\n    ) +\n  geom_boxplot(\n    width = 0.6,\n    outlier.shape = NA\n    ) + \n  ggbeeswarm::geom_beeswarm(\n    shape = 21,\n    width = 0.2\n    ) +\n  scale_y_continuous(\n    name = \"Body mass (g)\",\n    limits = c(0, 7000)\n    ) +\n  scale_x_discrete(\n   name = element_blank()\n  )"
  },
  {
    "objectID": "data_viz.html#choosing-a-theme",
    "href": "data_viz.html#choosing-a-theme",
    "title": "2  Data visualization",
    "section": "2.6 Choosing a theme",
    "text": "2.6 Choosing a theme\n\npenguins |&gt; \n  ggplot(\n    aes(x = species,\n        y = body_mass_g)\n    ) +\n  geom_boxplot(\n    width = 0.6,\n    outlier.shape = NA\n    ) + \n  ggbeeswarm::geom_beeswarm(\n    shape = 21,\n    width = 0.2\n    ) +\n  scale_y_continuous(\n    name = \"Body mass (g)\",\n    limits = c(0, 7000)\n    ) +\n  scale_x_discrete(\n   name = element_blank()\n   ) +\n  cowplot::theme_minimal_hgrid()\n\n\n\n\n\n\n\n\n\n\nWeird spacing\n\n\n\nOh no! Look at that brutal extra space between the zero line and the x-axis. You can remove that by adding expand = c(0, 0.5) to the scale_y_continous() function. This is particularly annoying on histograms.\n\n\n\npenguins |&gt; \n  ggplot(\n    aes(x = species,\n        y = body_mass_g)\n    ) +\n  geom_boxplot(\n    width = 0.5,\n    outlier.shape = NA\n    ) + \n  ggbeeswarm::geom_beeswarm(\n    shape = 21\n    ) +\n  scale_y_continuous(\n    name = \"Body mass (g)\",\n    limits = c(0, 7000),\n    expand = c(0, 0.5)\n    ) +\n  scale_x_discrete(\n   name = element_blank()\n   ) +\n  cowplot::theme_minimal_hgrid()"
  },
  {
    "objectID": "data_viz.html#the-small-details",
    "href": "data_viz.html#the-small-details",
    "title": "2  Data visualization",
    "section": "2.8 The small details",
    "text": "2.8 The small details\n\n2.8.1 Custom theme\n\n# Define a custom theme\ntheme_light_axis &lt;- function(color_default = \"grey30\") {\n    theme(\n      axis.title = element_text(color = color_default),\n      axis.text = element_text(color = color_default)\n    )\n}\n\n\npenguins |&gt; \n  ggplot(\n    aes(x = species,\n        y = body_mass_g)\n    ) +\n  geom_boxplot(\n    width = 0.5,\n    outlier.shape = NA\n    ) + \n  ggbeeswarm::geom_beeswarm(\n    shape = 21,\n    width = 0.2\n    ) +\n  scale_y_continuous(\n    name = \"Body mass (g)\",\n    limits = c(0, 7000),\n    expand = c(0, 0.5)\n    ) +\n  scale_x_discrete(\n   name = element_blank()\n   ) +\n  cowplot::theme_minimal_hgrid() +\n  theme_light_axis(color_default = \"grey50\")\n\n\n\n\n\npenguins |&gt; \n  ggplot(\n    aes(x = species,\n        y = body_mass_g/1000)\n    ) +\n  geom_boxplot(\n    width = 0.5,\n    outlier.shape = NA\n    ) + \n  ggbeeswarm::geom_beeswarm(\n    shape = 21,\n    width = 0.2\n    ) +\n  scale_y_continuous(\n    name = \"Body mass\",\n    limits = c(0, 7),\n    breaks = c(0, 2, 4, 6), \n    labels = c(\"0\", \"2 kg\", \"4 kg\", \"6 kg\"),\n    expand = c(0, 0)\n    ) +\n  scale_x_discrete(\n   name = element_blank()\n   ) +\n  cowplot::theme_minimal_hgrid() +\n  theme_light_axis(color_default = \"grey50\")\n\n\n\n\n\npenguins |&gt;  \n  ggplot(\n    aes(x = body_mass_g)\n    ) +\n  geom_histogram(\n    color = \"grey20\",\n    fill = \"grey80\"\n    ) +\n  facet_grid(rows = vars(island),\n             cols = vars(species)) +\n  cowplot::theme_minimal_hgrid() +\n  theme(strip.background = element_rect(fill = \"grey90\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`)."
  },
  {
    "objectID": "data_viz.html#footnotes",
    "href": "data_viz.html#footnotes",
    "title": "2  Data visualization",
    "section": "",
    "text": "Shhh! Don’t tell that to the Reproducibility Chapter of this workshop.↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Goodman, Steven N, Daniele Fanelli, and John P A Ioannidis. 2016.\n“What Does Research Reproducibility Mean?” Sci. Transl.\nMed. 8 (341): 341ps12.\n\n\nNational Academies of Sciences, Engineering, Medicine, Policy, Global\nAffairs, Committee on Science, Engineering, Medicine, Public Policy,\nBoard on Research Data, et al. 2019. Understanding Reproducibility\nand Replicability. National Academies Press (US).\n\n\nNoble, William Stafford. 2009. “A Quick Guide to Organizing\nComputational Biology Projects.” PLoS Comput. Biol. 5\n(7): e1000424.\n\n\nSandve, Geir Kjetil, Anton Nekrutenko, James Taylor, and Eivind Hovig.\n2013. “Ten Simple Rules for Reproducible Computational\nResearch.” PLoS Comput. Biol. 9 (10): e1003285.\n\n\nWickham, Hadley. 2014. “Tidy Data.” J. Stat.\nSoftw. 59 (September): 1–23."
  },
  {
    "objectID": "repro.html#projects-are-your-friend.",
    "href": "repro.html#projects-are-your-friend.",
    "title": "1  Reproducibility",
    "section": "1.5 Projects are your friend.",
    "text": "1.5 Projects are your friend."
  },
  {
    "objectID": "repro.html#avoid-magic-numbers",
    "href": "repro.html#avoid-magic-numbers",
    "title": "1  Reproducibility",
    "section": "1.7 Avoid magic numbers",
    "text": "1.7 Avoid magic numbers\nIt is a good idea to give numbers in your code a variable name. This makes it easier to know what that random number means.\nFor example, in the above code."
  },
  {
    "objectID": "repro.html#practice",
    "href": "repro.html#practice",
    "title": "3  Reproducibility",
    "section": "3.9 Practice",
    "text": "3.9 Practice\n\n\n\n\n\n\nLet’s practice these principles with a toy example\n\n\n\nThere are several things that could be better in this script. Note each area You don’t need to know exactly what each step does. But take this\nTry addressing these problems by splitting\n\n\n\n\n\nbig_unruly_script_height_weight_norm_plot_tso_01122022.R\n\n# ------------------------------------------------------------------------------\n# Script that runs through all my analyses for this project\n# TS O'Leary\n# ------------------------------------------------------------------------------\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(data.table)\nlibrary(drc)\n\n# Load data\n# Rather than actually loading local data let's just use the built-in starwars\n#   data for this and rename to dat as a place holder for dat &lt;- read_csv().\ndat &lt;- starwars \n\n# Normalize data \ndat2 &lt;- dat |&gt; \n  filter(!is.na(height)) |&gt; \n  group_by(species) |&gt; \n  mutate(norm_height = height/mean(height)) \n\n# Normalize data mass\ndat3 &lt;- dat |&gt; \n  filter(!is.na(mass)) |&gt; \n  group_by(species) |&gt; \n  mutate(norm_mass = height/mean(mass)) \n\n# Tall characters\ntall &lt;- dat |&gt; \n  filter(!is.na(height)) |&gt; \n  group_by(species) |&gt; \n  mutate(norm_height = height/mean(height)) |&gt; \n  filter(norm_height &gt; 1.1) |&gt; \n  arrange(desc(norm_height))\n\n# Short characters\nshort &lt;- dat |&gt; \n  filter(!is.na(height)) |&gt; \n  group_by(species) |&gt; \n  mutate(norm_height = height/mean(height)) |&gt; \n  filter(norm_height &lt; 0.9) |&gt; \n  arrange(desc(norm_height))\n\n# Create a histogram\ndat2 |&gt; \n  filter(!is.na(height) ) |&gt; \n  group_by(species) |&gt; \n  mutate(norm_height = height/mean(height)) |&gt; \n  ggplot() +\n  geom_histogram(aes(x = norm_height),\n                 color = \"grey20\",\n                 fill = \"grey80\") +\n  scale_y_continuous(expand = c(0, 0.05),\n                     name = \"Count\") +\n  scale_x_continuous(name = \"Normalized height\") +\n  cowplot::theme_half_open()\n\n# Create histogram\ndat2 |&gt; \n  filter(!is.na(height) ) |&gt; \n  group_by(species) |&gt; \n  mutate(norm_height = height/mean(height)) |&gt; \n  ggplot() +\n  geom_histogram(aes(x = log2(norm_height)),\n                 color = \"grey20\",\n                 fill = \"grey80\") +\n  scale_y_continuous(expand = c(0, 0.05),\n                     name = \"Count\") +\n  scale_x_continuous(name = \"log2(Normalized height)\") +\n  cowplot::theme_half_open()\n\n# Create a lolipop plot\ndat2 |&gt; \n  filter(norm_height != 1) |&gt; \n  ungroup(species) |&gt; \n  mutate(name = fct_reorder(name, norm_height)) |&gt; \n  ggplot(aes(y = name,\n             x = log2(norm_height),\n             fill = norm_height &lt; 1)) +\n  geom_segment(aes(xend = 0, yend = name,\n                   color = norm_height &lt; 1),\n               size = 1) +\n  geom_point(color = \"grey80\",\n             size = 5,\n             shape = 21) +\n  geom_vline(xintercept = 0, color = \"grey50\") +\n  scale_x_continuous(name = \"log2(Normalized height)\") +\n  scale_fill_manual(values = c(\"orchid\", \"skyblue\")) +\n  scale_color_manual(values = c(\"orchid\", \"skyblue\")) +\n  labs(y = element_blank()) +\n  cowplot::theme_minimal_vgrid() +\n  theme(legend.position = \"none\",\n        axis.line.y = element_blank())\n\n# Create another plot\nbind_rows(tall, short) |&gt; \n  ungroup(species) |&gt; \n  mutate(name = fct_reorder(name, norm_height)) |&gt; \n  ggplot(aes(y = name,\n             x = log2(norm_height),\n             fill = norm_height &lt; 1)) +\n  geom_segment(aes(xend = 0, yend = name,\n                   color = norm_height &lt; 1),\n               size = 1) +\n  geom_point(color = \"grey80\",\n             size = 5,\n             shape = 21) +\n  geom_vline(xintercept = 0, color = \"grey50\") +\n  scale_x_continuous(name = \"log2(Normalized height)\") +\n  scale_fill_manual(values = c(\"orchid\", \"skyblue\")) +\n  scale_color_manual(values = c(\"orchid\", \"skyblue\")) +\n  labs(y = element_blank()) +\n  cowplot::theme_minimal_vgrid() +\n  theme(legend.position = \"none\",\n        axis.line.y = element_blank())\n\n\n\n\n\n\n\n\nNotice the mistakes in the above script\n\n\n\n\n\n\nThe description of the script doesn’t explain what the script does.\nNone of the script output is saved. Neither the normalized data or plots.\nThere are several objects that are created. Some that aren’t used later.\nThere are multiple exploratory plots that are unsaved and unexplained and somewhat repetitive.\n\n\n\n\n\n3.9.1 Fix it!\n\n\n\nsrc/height/00_norm_data.R\n\n# ------------------------------------------------------------------------------\n# Log-normalize heights for each species in the starwars data set and filter to\n#   characters that are 10% taller or shorter than average within their species.\n# TS O'Leary\n# ------------------------------------------------------------------------------\n\n# Load libraries\nlibrary(tidyverse)\n\n# Load data\ndat &lt;- starwars \n\n# Minimum log2 normalized height\nmin_l2fc &lt;- 0.1\n\n# Normalize data and take the log2 of the normalized height\ndat &lt;- dat |&gt; \n  filter(!is.na(height)) |&gt; \n  group_by(species) |&gt; \n  mutate(norm_height = height/mean(height),\n         log2_norm_height = log2(norm_height)) |&gt; \n  ungroup(species) |&gt; \n  filter(abs(log2_norm_height) &gt; min_l2fc)\n\n# Save normalized data\nsaveRDS(dat, here::here(\"data/processed/starwars_norm.rds\"))\n\n\n\n\n\nsrc/height/01_plot_data.R\n\n# ------------------------------------------------------------------------------\n# Create a lolipop plot highlighting tall and short characters\n# TS O'Leary\n# ------------------------------------------------------------------------------\n\n# Load libraries\nlibrary(tidyverse)\n\n# Load data that has been log-normalized \ndat &lt;- readRDS(here::here(\"data/processed/starwars_norm.rds\"))\n\n# Variables for plotting \nheight_colors &lt;- c(\"orchid\", \"skyblue\")\n\n# Create another plot\ndat |&gt; \n  mutate(name = fct_reorder(name, log2_norm_height)) |&gt; \n  ggplot(aes(y = name,\n             x = log2_norm_height,\n             fill = log2_norm_height &lt; 0)) +\n  geom_segment(aes(xend = 0, yend = name,\n                   color = log2_norm_height &lt; 0),\n               size = 1) +\n  geom_point(color = \"grey80\",\n             size = 5,\n             shape = 21) +\n  geom_vline(xintercept = 0, color = \"grey50\") +\n  scale_x_continuous(name = \"log2(Normalized height)\") +\n  scale_fill_manual(values = height_colors) +\n  scale_color_manual(values = height_colors) +\n  labs(y = element_blank()) +\n  cowplot::theme_minimal_vgrid() +\n  theme(legend.position = \"none\",\n        axis.line.y = element_blank())\n\n# Save the plot\nggsave(here::here(\"output/figs/norm_height_lolipop.pdf\"),\n       height = 15,\n       width = 10,\n       units = \"cm\")\n\n\n\n\n\n\nGoodman, Steven N, Daniele Fanelli, and John P A Ioannidis. 2016. “What Does Research Reproducibility Mean?” Sci. Transl. Med. 8 (341): 341ps12.\n\n\nNational Academies of Sciences, Engineering, Medicine, Policy, Global Affairs, Committee on Science, Engineering, Medicine, Public Policy, Board on Research Data, et al. 2019. Understanding Reproducibility and Replicability. National Academies Press (US).\n\n\nNoble, William Stafford. 2009. “A Quick Guide to Organizing Computational Biology Projects.” PLoS Comput. Biol. 5 (7): e1000424.\n\n\nSandve, Geir Kjetil, Anton Nekrutenko, James Taylor, and Eivind Hovig. 2013. “Ten Simple Rules for Reproducible Computational Research.” PLoS Comput. Biol. 9 (10): e1003285."
  },
  {
    "objectID": "repro.html#quick-tools",
    "href": "repro.html#quick-tools",
    "title": "3  Reproducibility",
    "section": "3.7 Quick tools",
    "text": "3.7 Quick tools"
  },
  {
    "objectID": "repro.html#scratch-code",
    "href": "repro.html#scratch-code",
    "title": "3  Reproducibility",
    "section": "3.8 Scratch code",
    "text": "3.8 Scratch code\nI often find that I write code that is not used in the final analysis. You may find yourself doing the same. It is sometimes a random exploratory figure that doesn’t end up telling you much, or maybe you normalized some data in the wrong way, or used an inappropriate type of statistical model. But in each case, you have spent some valuable amount of time writing that code, and so you are reluctant to remove it from you script. So you just comment it out – or worse, just leave it hanging there in the script. After all, it might be useful down the line, somehow, somewhere. I sympathize with that, but I think it is worth removing all unnecessary code. It will help you in the future when you don’t remember what you did, and don’t know if that bit of code is important.\n\n\n\n\n\n\nA tip for removing all unecessary or redundant code\n\n\n\nIt can be hard to strip your code down to only the necessary bits, but it is worth it for the sake of clarity and reproducibility. I sometimes create a scratch.R file or a scratch/ directory where I copy and paste bits of code that I am reluctant to throw away. It helps clean up the final code and makes me feel a little less like I wasted my time."
  },
  {
    "objectID": "repro.html#tips",
    "href": "repro.html#tips",
    "title": "3  Reproducibility",
    "section": "3.7 Tips",
    "text": "3.7 Tips\n\n\n\n\n\n\nLet’s check out these quick tools\n\n\n\n\nStyler – an R Studio Addin that can automatically format your code to the tidyverse style guide or to some other custom style.\njanitor package – to clean up\n\n\n\n\n\n\n\n\n\nScratch code: A tip for removing all unecessary or redundant code\n\n\n\nI often find that I write code that is not used in the final analysis. You may find yourself doing the same. It is sometimes a random exploratory figure that doesn’t end up telling you much, or maybe you normalized some data in the wrong way, or used an inappropriate type of statistical model. But in each case, you have spent some valuable amount of time writing that code, and so you are reluctant to remove it from you script. So you just comment it out – or worse, just leave it hanging there in the script. After all, it might be useful down the line, somehow, somewhere. I sympathize with that, but I think it is worth removing all unnecessary code. It will help you in the future when you don’t remember what you did, and don’t know if that bit of code is important. It can be hard to strip your code down to only the necessary bits, but it is worth it for the sake of clarity and reproducibility. I sometimes create a scratch.R file or a scratch/ directory where I copy and paste bits of code that I am reluctant to throw away. It helps clean up the final code and makes me feel a little less like I wasted my time."
  },
  {
    "objectID": "repro.html#tools",
    "href": "repro.html#tools",
    "title": "3  Reproducibility",
    "section": "3.7 Tools",
    "text": "3.7 Tools\n\n3.7.1 Snippets\nYou should use the available tools as much as you can to aid your workflow.\nI use snippets to create my script templates.\n\nsnippet mhead_snip\n    # ------------------------------------------------------------------------------\n    # ${1:script_description}\n    # TS O'Leary\n    # ------------------------------------------------------------------------------\n\n    # Load libraries\n    library(tidyverse)\n\n    # Load data\n    dat &lt;- read_csv(here::here(\"data/raw/starwars.csv\"))\n    \n    # Analyze data\n    dat &lt;- dat %&gt;%\n    group_by(Species) %&gt;%\n        count()\n    \n    # Save data\n    saveRDS(here::here(\"data/processed/count.rds\"))\n\nThis template ensures that I do several things:\n\nAdd a top level description to each file. I usually try to keep it to one sentance that says what the script is doing. If you have split up your scripts into bite sized chunks, this should be easy.\nGives a place to load libraries and data at the top of the script.\nReminds me to save the output at the end of the script.\n\n\n\n\n\n\n\nLet’s check out these quick tools\n\n\n\n\nStyler – an R Studio Addin that can automatically format your code to the tidyverse style guide or to some other custom style.\njanitor package – to clean up\n\n\n\n\n\n\n\n\n\nScratch code: A tip for removing all unecessary or redundant code\n\n\n\nI often find that I write code that is not used in the final analysis. You may find yourself doing the same. It is sometimes a random exploratory figure that doesn’t end up telling you much, or maybe you normalized some data in the wrong way, or used an inappropriate type of statistical model. But in each case, you have spent some valuable amount of time writing that code, and so you are reluctant to remove it from you script. So you just comment it out – or worse, just leave it hanging there in the script. After all, it might be useful down the line, somehow, somewhere. I sympathize with that, but I think it is worth removing all unnecessary code. It will help you in the future when you don’t remember what you did, and don’t know if that bit of code is important. It can be hard to strip your code down to only the necessary bits, but it is worth it for the sake of clarity and reproducibility. I sometimes create a scratch.R file or a scratch/ directory where I copy and paste bits of code that I am reluctant to throw away. It helps clean up the final code and makes me feel a little less like I wasted my time."
  },
  {
    "objectID": "tidyverse.html#bam-that-is-tidy-data.",
    "href": "tidyverse.html#bam-that-is-tidy-data.",
    "title": "1  Tidyverse",
    "section": "1.3 BAM! That is tidy data.",
    "text": "1.3 BAM! That is tidy data.\n\nEvery column is a variable – id, time, height.\nEvery row is an observation – a height measurement on a single plant.\nEvery cell is a single value."
  },
  {
    "objectID": "tidyverse.html#tidy-data-1",
    "href": "tidyverse.html#tidy-data-1",
    "title": "1  Tidyverse",
    "section": "1.7 Tidy data",
    "text": "1.7 Tidy data\n\nConvert billboard to tidy data\n\n# Print billboard data form the year 2000\ntidyr::billboard\n\n# A tibble: 317 × 79\n   artist track date.ent…¹   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8   wk9\n   &lt;chr&gt;  &lt;chr&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac  Baby… 2000-02-26    87    82    72    77    87    94    99    NA    NA\n 2 2Ge+h… The … 2000-09-02    91    87    92    NA    NA    NA    NA    NA    NA\n 3 3 Doo… Kryp… 2000-04-08    81    70    68    67    66    57    54    53    51\n 4 3 Doo… Loser 2000-10-21    76    76    72    69    67    65    55    59    62\n 5 504 B… Wobb… 2000-04-15    57    34    25    17    17    31    36    49    53\n 6 98^0   Give… 2000-08-19    51    39    34    26    26    19     2     2     3\n 7 A*Tee… Danc… 2000-07-08    97    97    96    95   100    NA    NA    NA    NA\n 8 Aaliy… I Do… 2000-01-29    84    62    51    41    38    35    35    38    38\n 9 Aaliy… Try … 2000-03-18    59    53    38    28    21    18    16    14    12\n10 Adams… Open… 2000-08-26    76    76    74    69    68    67    61    58    57\n# … with 307 more rows, 67 more variables: wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;,\n#   wk43 &lt;dbl&gt;, wk44 &lt;dbl&gt;, wk45 &lt;dbl&gt;, wk46 &lt;dbl&gt;, wk47 &lt;dbl&gt;, wk48 &lt;dbl&gt;, …\n\n\n\n\nClick for hidden solution\n# Load libraries\nlibrary(tidyverse)\n\n# Convert to tidy format\nbillboard |&gt; \n  pivot_longer(contains(\"wk\"),\n               names_to = \"week\",\n               values_to = \"rank\") |&gt; \n  mutate(week = as.numeric(str_remove_all(week, \"wk\")))\n\n\n\n\n\n\n\n\nreadr package\n\n\n\n\n\nWe won’t spend much time here importing data, but it is worth thinking about and I do reccomend the tidyverse functions here too. I prefer readr::read_csv() over read.csv() from base R. And same with readr::write_csv() over the base version8.\nWhile importing data with readr::read_csv(), I find it a lot easier to define the data structure as you import data. For example, setting specific columns as a factor and setting the levels is easier to do with tidyverse functions. It also makes it so you are already working with a tibble instead of data.frame.\n\n\n\n\n\n\n\n\n\nconflicted package\n\n\n\n\n\nGreat way to explicitly tell R which function to prefer by default when you have function names being masked by different packages.\nCheck it out!\n\n\n\n\n\n\n\n\n\nglue package\n\n\n\n\n\nA small package I ran across that allows for a cool alternative to paste() when you need to use named variables. For example, you can use name &lt;- \"Thomas\"; glue::glue(\"My name is {name}.\") as an alternative to name &lt;- \"Thomas\"; paste0(\"My name is \", name, \".\"), which you have to be careful about the spacing. And as what your goal gets more complicated, it gets more and more useful. Check it out!\n\n\n\n\n\n\n\nWickham, Hadley. 2014. “Tidy Data.” J. Stat. Softw. 59 (September): 1–23."
  },
  {
    "objectID": "tidyverse.html#basic-tidyverse-and-the-core-verbs",
    "href": "tidyverse.html#basic-tidyverse-and-the-core-verbs",
    "title": "1  Tidyverse",
    "section": "1.3 Basic tidyverse and the core verbs",
    "text": "1.3 Basic tidyverse and the core verbs\n\n1.3.1 Intro to dplyr\ndplyr has a few core functions that are built to work on\nTaken from the dplyr vignette.\n\n1.3.1.1 Rows\n\nfilter() chooses rows based on column values.\nslice() chooses rows based on location.\narrange() changes the order of the rows.\n\n\n\n1.3.1.2 Columns\n\nselect() changes whether or not a column is included.\nrename() changes the name of columns.\nmutate() changes the values of columns and creates new columns.\nrelocate() changes the order of the columns.\n\n\n\n1.3.1.3 Groups of rows\n\nsummarise() collapses a group into a single row.\n\n\n\n\n1.3.2 Pipe %&gt;%\nFrom magrittr.\n\nx %&gt;% f is equivalent to f(x)\nx %&gt;% f(y) is equivalent to f(x, y)\nx %&gt;% f %&gt;% g %&gt;% h is equivalent to h(g(f(x)))\n\nThe argument placeholder\n\nx %&gt;% f(y, .) is equivalent to f(y, x)\nx %&gt;% f(y, z = .) is equivalent to f(y, z = x)\n\n\n\n\n\n\n\n|&gt; native pipe operator\n\n\n\n\n\nIn new versions of R (&gt; 4.1.0), there is now a native pipe operator that comes within the syntax of R itself, rather than being loaded as a part of a separate package.\nAlthough they are largely the same, there are some meaningful differences between the two. These two articles from the tidyverse and Isabella Velásquez do a good job of explaining the differences relevant differences.\nI have started just using the native pipe (|&gt;) for almost all cases now. First, becuase it just looks cleaner. But also because then you can use a pipe without explicitly loading the mattingr package (_e.g. iris |&gt; dplyr::glimpse()). Where if you had used the magrittr pipe (%&gt;%), but not loaded the magrittr package, you would get an error.\n\n\n\n\n1.3.2.1 A fun example from R for data science\n\nfoo_foo &lt;- little_bunny()\nfoo_foo_1 &lt;- hop(foo_foo, through = forest)\nfoo_foo_2 &lt;- scoop(foo_foo_1, up = field_mice)\nfoo_foo_3 &lt;- bop(foo_foo_2, on = head)\n\n\nfoo_foo |&gt;\n  hop(through = forest) |&gt;\n  scoop(up = field_mice) |&gt;\n  bop(on = head)\n\n\n\n\n1.3.3 arrange\n\n# dat |&gt;\n#   arrange(tissue, iu_gfw)\n\n\n# dat |&gt;\n#   arrange(desc(tissue), desc(iu_gfw))\n\n\n\n1.3.4 summarise\n\n# dat |&gt;\n#   summarise(iu_gfw_avg = mean(iu_gfw))\n\n\n\n1.3.5 group_by\n\n# dat |&gt;\n#   group_by(tissue) |&gt;\n#   summarise(iu_gfw_avg = mean(iu_gfw),\n#             iu_gfw_sd = sd(iu_gfw))\n\nWarning that you must be careful about the order when reusing variable names.\n\n# # Bad order\n# dat |&gt;\n#   group_by(tissue) |&gt;\n#   summarise(iu_gfw = mean(iu_gfw),\n#             sd = sd(iu_gfw))"
  },
  {
    "objectID": "tidyverse.html#introduction-to-dplyr-and-data-wrangling",
    "href": "tidyverse.html#introduction-to-dplyr-and-data-wrangling",
    "title": "1  Tidyverse",
    "section": "1.3 Introduction to dplyr and data wrangling",
    "text": "1.3 Introduction to dplyr and data wrangling\nThe package dplyr has a few core functions (or verbs) that are built to work on tidy data. We will check out each briefly. But before we do, let’s introduce the sometimes confusing by often useful pipe operator.\n\n\n\n\n\n\nCore verbs\n\n\n\n\n\n\n1.3.0.1 Rows\n\nfilter() chooses rows based on column values.\nslice() chooses rows based on location.\narrange() changes the order of the rows.\n\n\n\n1.3.0.2 Columns\n\nselect() changes whether or not a column is included.\nrename() changes the name of columns.\nmutate() changes the values of columns and creates new columns.\nrelocate() changes the order of the columns.\n\n\n\n1.3.0.3 Groups of rows\n\nsummarise() collapses a group into a single row.\n\n\n\n\n\n\nPipe %&gt;%\nThe pipe operator is a more human-readable alternative to nesting function calls within each other. As the magrittr vignette explains, it makes function calls more naturally reflect the true order of operations. Here are the abstract examples of using a pipe.\n\nx %&gt;% f is equivalent to f(x)\nx %&gt;% f(y) is equivalent to f(x, y)\nx %&gt;% f %&gt;% g %&gt;% h is equivalent to h(g(f(x)))\n\nThe argument placeholder5\n\nx %&gt;% f(y, .) is equivalent to f(y, x)\nx %&gt;% f(y, z = .) is equivalent to f(y, z = x)\n\n\n\n\n\n\n\n|&gt; native pipe operator\n\n\n\n\n\nIn new versions of R (&gt; 4.1.0), there is now a native pipe operator (|&gt;) that comes within the syntax of R itself, rather than being loaded as a part of a separate package.\nAlthough they are largely the same, there are some meaningful differences between the two. These two articles from the tidyverse and Isabella Velásquez do a good job of explaining the differences relevant differences.\nI have started just using the native pipe (|&gt;) for almost all cases now. Trivially because it just looks cleaner. But also because using the native pipe allows you to use a pipe without explicitly loading the mattingr package. For example iris |&gt; head() just works, where if you instead had used the magrittr pipe (%&gt;%), but not loaded the magrittr package (or the whole tidverse), you would get an error.\nI will use the native pipe in almost all instances for the rest of this workshop.\n\n\n\n\n1.3.0.4 A fun example from R for data science\n\n# As a series of statements with intermediate objects\nfoo_foo &lt;- little_bunny()\nfoo_foo_1 &lt;- hop(foo_foo, through = forest)\nfoo_foo_2 &lt;- scoop(foo_foo_1, up = field_mice)\nfoo_foo_3 &lt;- bop(foo_foo_2, on = head)\n\n\n# As a bunch of nested function calls\nfoo_foo &lt;- bop(scoop(hop(little_bunny(), through = forest), up = field_mice), on = head)\n\n\n# With the pipe!\nfoo_foo &lt;- foo_foo |&gt;\n  hop(through = forest) |&gt;\n  scoop(up = field_mice) |&gt;\n  bop(on = head)\n\n\n\n\n\n\n\nThis last way of coding is much more human readable! Both the order of operations and which arguments belong to which function are clear. And it doesn’t clog up the global environment with a bunch of meaningless intermediate objects."
  },
  {
    "objectID": "tidyverse.html#more-advanced-techniques",
    "href": "tidyverse.html#more-advanced-techniques",
    "title": "1  Tidyverse",
    "section": "1.5 More advanced techniques",
    "text": "1.5 More advanced techniques\nthe core verbs useful for first exp but often hoping to do more complex things. Run statistical models etc.\n\nOther useful tidyverse packages\n\nbroom – clean model output – technically a core package within tidymodels\ntidymodels – https://www.tidymodels.org/packages/\nrvest – web scraping – mining data from a website\nmodelr – modelling – support for modelling data in the tidyverse"
  },
  {
    "objectID": "tidyverse.html#more-complex-data",
    "href": "tidyverse.html#more-complex-data",
    "title": "1  Tidyverse",
    "section": "1.6 More complex data",
    "text": "1.6 More complex data\nFor the most part, I find myself working with 2D structured data (i.e., table with rows and columns). But sometimes, when you are working with complex experimental designs (e.g., multiple factors) it can be useful to try nesting your data.\n\n\n\n\n\n\nLet’s use the ChickWeight data as a quick example\n\n\n\n\ndat &lt;- ChickWeight |&gt; \n  janitor::clean_names() |&gt; \n  as_tibble()\n\ndat |&gt;\n  glimpse()\n\nRows: 578\nColumns: 4\n$ weight &lt;dbl&gt; 42, 51, 59, 64, 76, 93, 106, 125, 149, 171, 199, 205, 40, 49, 5…\n$ time   &lt;dbl&gt; 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 21, 0, 2, 4, 6, 8, 10, 1…\n$ chick  &lt;ord&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ diet   &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\n\n\ndat |&gt;\n  group_by(chick, diet) |&gt;\n  nest() \n\n# A tibble: 50 × 3\n# Groups:   chick, diet [50]\n   chick diet  data             \n   &lt;ord&gt; &lt;fct&gt; &lt;list&gt;           \n 1 1     1     &lt;tibble [12 × 2]&gt;\n 2 2     1     &lt;tibble [12 × 2]&gt;\n 3 3     1     &lt;tibble [12 × 2]&gt;\n 4 4     1     &lt;tibble [12 × 2]&gt;\n 5 5     1     &lt;tibble [12 × 2]&gt;\n 6 6     1     &lt;tibble [12 × 2]&gt;\n 7 7     1     &lt;tibble [12 × 2]&gt;\n 8 8     1     &lt;tibble [11 × 2]&gt;\n 9 9     1     &lt;tibble [12 × 2]&gt;\n10 10    1     &lt;tibble [12 × 2]&gt;\n# … with 40 more rows\n\n\n\ndat_nest &lt;- dat |&gt;\n  group_by(chick, diet) |&gt;\n  nest() \n\ndat_nest\n\n# A tibble: 50 × 3\n# Groups:   chick, diet [50]\n   chick diet  data             \n   &lt;ord&gt; &lt;fct&gt; &lt;list&gt;           \n 1 1     1     &lt;tibble [12 × 2]&gt;\n 2 2     1     &lt;tibble [12 × 2]&gt;\n 3 3     1     &lt;tibble [12 × 2]&gt;\n 4 4     1     &lt;tibble [12 × 2]&gt;\n 5 5     1     &lt;tibble [12 × 2]&gt;\n 6 6     1     &lt;tibble [12 × 2]&gt;\n 7 7     1     &lt;tibble [12 × 2]&gt;\n 8 8     1     &lt;tibble [11 × 2]&gt;\n 9 9     1     &lt;tibble [12 × 2]&gt;\n10 10    1     &lt;tibble [12 × 2]&gt;\n# … with 40 more rows\n\ndat_nest$data[1]\n\n[[1]]\n# A tibble: 12 × 2\n   weight  time\n    &lt;dbl&gt; &lt;dbl&gt;\n 1     42     0\n 2     51     2\n 3     59     4\n 4     64     6\n 5     76     8\n 6     93    10\n 7    106    12\n 8    125    14\n 9    149    16\n10    171    18\n11    199    20\n12    205    21\n\n\n\nbroom\n\n# Load library\nlibrary(broom)\n\nCheck out the broom vignette.\nAnd the broom and dplyr vignette.\ntidy: constructs a tibble that summarizes the model’s statistical findings. This includes coefficients and p-values for each term in a regression, per-cluster information in clustering applications, or per-test information for multtest functions.\nglance: construct a concise one-row summary of the model. This typically contains values such as R^2, adjusted R^2, and residual standard error that are computed once for the entire model.\n\nlinear_model_results &lt;- dat |&gt;\n  group_by(chick, diet) |&gt;\n  nest() |&gt;\n  mutate(\n    fit = map(data, ~ lm(weight ~ time, data = .x)),\n    tidied = map(fit, tidy),\n    glanced = map(fit, glance)\n  ) |&gt; \n  unnest(tidied) \n\nlinear_model_results\n\n# A tibble: 100 × 10\n# Groups:   chick, diet [50]\n   chick diet  data     fit    term    estim…¹ std.e…² stati…³  p.value glanced \n   &lt;ord&gt; &lt;fct&gt; &lt;list&gt;   &lt;list&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;list&gt;  \n 1 1     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   24.5    6.73     3.64 4.56e- 3 &lt;tibble&gt;\n 2 1     1     &lt;tibble&gt; &lt;lm&gt;   time       7.99   0.524   15.3  2.97e- 8 &lt;tibble&gt;\n 3 2     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   24.7    4.93     5.01 5.26e- 4 &lt;tibble&gt;\n 4 2     1     &lt;tibble&gt; &lt;lm&gt;   time       8.72   0.384   22.7  6.15e-10 &lt;tibble&gt;\n 5 3     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   23.2    5.08     4.56 1.04e- 3 &lt;tibble&gt;\n 6 3     1     &lt;tibble&gt; &lt;lm&gt;   time       8.49   0.396   21.5  1.08e- 9 &lt;tibble&gt;\n 7 4     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   32.9    4.01     8.21 9.42e- 6 &lt;tibble&gt;\n 8 4     1     &lt;tibble&gt; &lt;lm&gt;   time       6.09   0.312   19.5  2.70e- 9 &lt;tibble&gt;\n 9 5     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   16.9    7.56     2.24 4.93e- 2 &lt;tibble&gt;\n10 5     1     &lt;tibble&gt; &lt;lm&gt;   time      10.1    0.588   17.1  9.88e- 9 &lt;tibble&gt;\n# … with 90 more rows, and abbreviated variable names ¹​estimate, ²​std.error,\n#   ³​statistic\n\n\n\nlinear_model_results |&gt; \n  filter(term == \"time\") |&gt; \n  group_by(diet) |&gt; \n  summarize(avg_slope = mean(estimate))\n\n# A tibble: 4 × 2\n  diet  avg_slope\n  &lt;fct&gt;     &lt;dbl&gt;\n1 1          5.85\n2 2          8.61\n3 3         11.4 \n4 4          9.52\n\n\n\ndat |&gt;\n  ggplot() +\n  geom_line(aes(x = time, \n                y = weight,\n                group = chick)) +\n  facet_wrap(~ diet) +\n  cowplot::theme_minimal_hgrid() +\n  scale_y_continuous(limits = c(0, 400),\n                     expand = c(0, 0.5)) +\n  theme(strip.background = element_rect(fill = \"grey80\"))"
  },
  {
    "objectID": "tidyverse.html#core-verbs-1",
    "href": "tidyverse.html#core-verbs-1",
    "title": "1  Tidyverse",
    "section": "1.4 Core verbs",
    "text": "1.4 Core verbs\nNow let’s use the starwars data from the dplyr package to explore the some of the core functions of the tidyverse.\n\ndplyr::starwars\n\n# A tibble: 87 × 14\n   name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n   &lt;chr&gt;        &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  \n 1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n 2 C-3PO          167    75 &lt;NA&gt;    gold    yellow    112   none  mascu… Tatooi…\n 3 R2-D2           96    32 &lt;NA&gt;    white,… red        33   none  mascu… Naboo  \n 4 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n 5 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n 6 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n 7 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n 8 R5-D4           97    32 &lt;NA&gt;    white,… red        NA   none  mascu… Tatooi…\n 9 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n10 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n# … with 77 more rows, 4 more variables: species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;, and abbreviated variable names\n#   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n\n\nstarwars |&gt;\n  arrange(height)\n\n# A tibble: 87 × 14\n   name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n   &lt;chr&gt;        &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  \n 1 Yoda            66    17 white   green   brown       896 male  mascu… &lt;NA&gt;   \n 2 Ratts Tyer…     79    15 none    grey, … unknown      NA male  mascu… Aleen …\n 3 Wicket Sys…     88    20 brown   brown   brown         8 male  mascu… Endor  \n 4 Dud Bolt        94    45 none    blue, … yellow       NA male  mascu… Vulpter\n 5 R2-D2           96    32 &lt;NA&gt;    white,… red          33 none  mascu… Naboo  \n 6 R4-P17          96    NA none    silver… red, b…      NA none  femin… &lt;NA&gt;   \n 7 R5-D4           97    32 &lt;NA&gt;    white,… red          NA none  mascu… Tatooi…\n 8 Sebulba        112    40 none    grey, … orange       NA male  mascu… Malast…\n 9 Gasgano        122    NA none    white,… black        NA male  mascu… Troiken\n10 Watto          137    NA black   blue, … yellow       NA male  mascu… Toydar…\n# … with 77 more rows, 4 more variables: species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;, and abbreviated variable names\n#   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n\n\nstarwars |&gt;\n  arrange(desc(height))\n\n# A tibble: 87 × 14\n   name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n   &lt;chr&gt;        &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  \n 1 Yarael Poof    264    NA none    white   yellow     NA   male  mascu… Quermia\n 2 Tarfful        234   136 brown   brown   blue       NA   male  mascu… Kashyy…\n 3 Lama Su        229    88 none    grey    black      NA   male  mascu… Kamino \n 4 Chewbacca      228   112 brown   unknown blue      200   male  mascu… Kashyy…\n 5 Roos Tarpa…    224    82 none    grey    orange     NA   male  mascu… Naboo  \n 6 Grievous       216   159 none    brown,… green,…    NA   male  mascu… Kalee  \n 7 Taun We        213    NA none    grey    black      NA   fema… femin… Kamino \n 8 Rugor Nass     206    NA none    green   orange     NA   male  mascu… Naboo  \n 9 Tion Medon     206    80 none    grey    black      NA   male  mascu… Utapau \n10 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n# … with 77 more rows, 4 more variables: species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;, and abbreviated variable names\n#   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n\n\nstarwars |&gt;\n  summarise(height_avg = mean(height, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  height_avg\n       &lt;dbl&gt;\n1       174.\n\n\n\n# Alternatively, you could filter out any rows without a height recorded\nstarwars |&gt;\n  filter(!is.na(height)) |&gt; \n  summarise(height_avg = mean(height))\n\n# A tibble: 1 × 1\n  height_avg\n       &lt;dbl&gt;\n1       174.\n\n\n\nstarwars |&gt;\n  group_by(species) |&gt;\n  summarise(height_avg = mean(height, na.rm = TRUE))\n\n# A tibble: 38 × 2\n   species   height_avg\n   &lt;chr&gt;          &lt;dbl&gt;\n 1 Aleena           79 \n 2 Besalisk        198 \n 3 Cerean          198 \n 4 Chagrian        196 \n 5 Clawdite        168 \n 6 Droid           131.\n 7 Dug             112 \n 8 Ewok             88 \n 9 Geonosian       183 \n10 Gungan          209.\n# … with 28 more rows\n\n\n\nstarwars |&gt;\n  group_by(species) |&gt;\n  summarise(height_avg = mean(height, na.rm = TRUE)) |&gt; \n  arrange(desc(height_avg))\n\n# A tibble: 38 × 2\n   species  height_avg\n   &lt;chr&gt;         &lt;dbl&gt;\n 1 Quermian       264 \n 2 Wookiee        231 \n 3 Kaminoan       221 \n 4 Kaleesh        216 \n 5 Gungan         209.\n 6 Pau'an         206 \n 7 Besalisk       198 \n 8 Cerean         198 \n 9 Chagrian       196 \n10 Nautolan       196 \n# … with 28 more rows\n\n\n\n\n\n\n\n\nPractice!\n\n\n\nNow let’s spend five minutes putting this to practice. Check out the data wrangling cheat sheet for more functions. Or better yet, try to think of something you want to do with your data and then look it up and do it.\nYou can also try these on other data sets! Calling data() in your console will show a bunch of pre-loaded data sets ."
  },
  {
    "objectID": "tidyverse.html#a-step-further",
    "href": "tidyverse.html#a-step-further",
    "title": "1  Tidyverse",
    "section": "1.5 A step further",
    "text": "1.5 A step further\nThese core verbs and other data wrangling functions are useful for the first steps of your data exploration, but often you are ultimately aiming to do something complex than calculating a per group mean or standard deviation. For example, you may want to run a linear model or an analysis of variance on your data. The good news is you can do this with help from the tidyverse!\nBecause people often learn how to do a linear regression or an analysis of variance in base R and the output of the model is often non-standard format, they never realize that there are tidy alternatives! Let’s briefly look at the base R way of doing things and then see how tidyverse packages (e.g., broom, dplyr, and purrr)\n\n\n\n\n\n\nLet’s use the ChickWeight data as a quick example.\n\n\n\nQuick note: We will run some statistical tests (e.g., a linear regression and an ANOVA) on this data. There is some reason to think that these specific tests may not be best choice for this data (e.g., growth rate over time may not be linear6). But because this is not a statistics workshop, we will concern ourselves primarily with the mechanics of the coding, and save the worry about the specifics of the model for some other time.\n\n\nBefore we do anything, let’s take a quick look at the data.\n\nAverage per dietEach chick individually\n\n\n\n\nCode\ndat &lt;- ChickWeight |&gt;\n  mutate(diet_name = case_when(Diet == 1 ~ \" Control diet\",\n                               Diet == 2 ~ \" 10% protein added\",\n                               Diet == 3 ~ \" 20% protein added\",\n                               Diet == 4 ~ \" 30% protein added\")) |&gt; \n  group_by(diet_name, Time) |&gt; \n  summarise(weight = mean(weight),\n            .groups = \"drop\")\n  \ndat |&gt;  \n  ggplot(aes(x = Time,\n             y = weight,\n             color = diet_name,\n             linetype = diet_name)) +\n  geom_line(linewidth = 1) +\n  geom_text(data = dat |&gt; \n              filter(Time == 21),\n            aes(label = diet_name),\n            hjust = 0,\n            nudge_x = 0) +\n  scale_color_manual(name = element_blank(), \n                     values = c(\"grey80\",\n                                \"orange\",\n                                \"grey80\",\n                                \"grey30\")) +\n  scale_y_continuous(limits = c(0, 300),\n                     expand = c(0, 0.5),\n                     minor_breaks = c(50, 150, 250),\n                     name = \"Average mass (g)\") +\n  scale_x_continuous(limits = c(0, 21),\n                     expand = c(0, 0),\n                     breaks = c(0, 7, 14, 21),\n                     name = \"Time (days)\") +\n  scale_linetype_manual(values = c(1, 1, 1, 2)) +\n  coord_cartesian(clip = \"off\", xlim = c(0, 21)) +\n  cowplot::theme_minimal_hgrid() +\n  theme(legend.position = \"none\",\n        plot.margin = unit(c(1, 7, 1, 1), \"lines\"),\n        panel.grid.minor.y = element_line(color = \"grey90\", \n                                          linetype = 2)) \n\n\n\n\n\n\n\n\n\nCode\nChickWeight |&gt;\n  mutate(diet_name = case_when(Diet == 1 ~ \"Control diet\",\n                               Diet == 2 ~ \"Protein added (10%)\",\n                               Diet == 3 ~ \"Protein added (20%)\",\n                               Diet == 4 ~ \"Protein added (30%)\")) |&gt; \n  ggplot() +\n  geom_line(aes(x = Time, \n                y = weight,\n                group = Chick)) +\n  facet_wrap(~ diet_name) +\n  cowplot::theme_minimal_hgrid() +\n  scale_y_continuous(limits = c(0, 400),\n                     expand = c(0, 0.5),\n                     name = \"mass (g)\") +\n  labs(x = \"Time (days)\") +\n  theme(strip.background = element_rect(fill = \"grey80\"))\n\n\n\n\n\n\n\n\nSo let’s say you wanted to know the growth rate for each chick7. The base R approach would be to take each indi\n\n# Take the data for chick one\nchick_1 &lt;- ChickWeight |&gt; \n  filter(Chick == 1)\n\n# Print out data for Chick 1\nchick_1\n\n   weight Time Chick Diet\n1      42    0     1    1\n2      51    2     1    1\n3      59    4     1    1\n4      64    6     1    1\n5      76    8     1    1\n6      93   10     1    1\n7     106   12     1    1\n8     125   14     1    1\n9     149   16     1    1\n10    171   18     1    1\n11    199   20     1    1\n12    205   21     1    1\n\n\n\n# Run a linear model on chick 1 data\nlm_mod &lt;- lm(weight ~ Time, data = chick_1)\n\n# Print results\nlm_mod\n\n\nCall:\nlm(formula = weight ~ Time, data = chick_1)\n\nCoefficients:\n(Intercept)         Time  \n     24.465        7.988  \n\n# Print summary of results with more information\nsummary(lm_mod)\n\n\nCall:\nlm(formula = weight ~ Time, data = chick_1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.3202 -11.3081  -0.3444  11.1162  17.5346 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  24.4654     6.7279   3.636  0.00456 ** \nTime          7.9879     0.5236  15.255 2.97e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.29 on 10 degrees of freedom\nMultiple R-squared:  0.9588,    Adjusted R-squared:  0.9547 \nF-statistic: 232.7 on 1 and 10 DF,  p-value: 2.974e-08\n\n\nBut the brutal part of this base R workflow is that it requires you to work with this non-standard formatted object “lm”.\n\nclass(lm_mod)\n\n[1] \"lm\"\n\n\nIf you wanted to extract the specific slope you would have to use lm_mod$coefficients[\"Time\"] or for the R-squared: summary(lm_mod)$r.squared, or even worse, the p-value associated with the slope is a wild call. summary(lm_mod)$coefficients[\"Time\",4]. These are really a nightmare to extract, which might not be a huge problem if you are just running one model. But if you are running several or more models and you want to filter down those results to certain criteria (e.g., p-value &lt; 0.05 or R2 &gt; 0.1), it would be a real headache. But there are two simple functions in the broom packages that make getting that information easy! Check them out!\n\n1.5.1 Tidy alternative\n\n\n\n\n\n\nTwo useful broom functions (Info from here)\n\n\n\n\ntidy(): constructs a tibble that summarizes the model’s statistical findings. This includes coefficients and p-values for each term in a regression, per-cluster information in clustering applications, or per-test information for multtest functions.\n\n- glance(): constructs a concise one-row summary of the model. This typically contains values such as R2, adjusted R2, and residual standard error that are computed once for the entire model.\n\n\n\n# Tidy output of linear model\nbroom::tidy(lm_mod)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)    24.5      6.73       3.64 0.00456     \n2 Time            7.99     0.524     15.3  0.0000000297\n\n\n\n# Glance output of linear model\nbroom::glance(lm_mod)\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;int&gt;\n1   0.959   0.955  12.3    233. 2.97e-8     1  -46.0  98.1  99.5   1511.      10\n# … with 1 more variable: nobs &lt;int&gt;, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n\n\n\n1.5.2 Take it to the next level with purrr\nThat approach is a good supplement to the base R workflow. You can possibly imagine doing a large for loop to go through all the individual chicks one by one. And that might work fine, but it is a bit of a hassle.\n\n\nClick here for a quick for loop example\n# Create an empty list to populate results\nlm_mod_all &lt;- list()\n\n# Loop through all chicks\nfor (i in unique(ChickWeight$Chick)) {\n lm_mod_all[[i]] &lt;- lm(\n   weight ~ Time, \n   data = subset(ChickWeight, Chick == i)\n  )\n}\n\n# Print a result from example 25\nsummary(lm_mod_all$`25`)\n\n\nFortunately, there are alternatives within the tidyverse!\n\n# Load library\nlibrary(broom)\n\n# Nest the data \nChickWeight |&gt;\n  group_by(Chick, Diet) |&gt;\n  nest() \n\n# A tibble: 50 × 3\n# Groups:   Chick, Diet [50]\n   Chick Diet  data             \n   &lt;ord&gt; &lt;fct&gt; &lt;list&gt;           \n 1 1     1     &lt;tibble [12 × 2]&gt;\n 2 2     1     &lt;tibble [12 × 2]&gt;\n 3 3     1     &lt;tibble [12 × 2]&gt;\n 4 4     1     &lt;tibble [12 × 2]&gt;\n 5 5     1     &lt;tibble [12 × 2]&gt;\n 6 6     1     &lt;tibble [12 × 2]&gt;\n 7 7     1     &lt;tibble [12 × 2]&gt;\n 8 8     1     &lt;tibble [11 × 2]&gt;\n 9 9     1     &lt;tibble [12 × 2]&gt;\n10 10    1     &lt;tibble [12 × 2]&gt;\n# … with 40 more rows\n\n\n\n\n\n\n\n\nCaution\n\n\n\nNotice that by doing the nesting we are violating Rule 3 of tidy data – every cell is a single value. But that’s okay because we are just doing that temporarily so we can run a linear model on the data nested within each row.\n\n\n\n# Create a nested data frame to check out the object\ndat_nest &lt;- ChickWeight |&gt;\n  group_by(Chick, Diet) |&gt;\n  nest() \n\n# Print out the new nested data\ndat_nest\n\n# A tibble: 50 × 3\n# Groups:   Chick, Diet [50]\n   Chick Diet  data             \n   &lt;ord&gt; &lt;fct&gt; &lt;list&gt;           \n 1 1     1     &lt;tibble [12 × 2]&gt;\n 2 2     1     &lt;tibble [12 × 2]&gt;\n 3 3     1     &lt;tibble [12 × 2]&gt;\n 4 4     1     &lt;tibble [12 × 2]&gt;\n 5 5     1     &lt;tibble [12 × 2]&gt;\n 6 6     1     &lt;tibble [12 × 2]&gt;\n 7 7     1     &lt;tibble [12 × 2]&gt;\n 8 8     1     &lt;tibble [11 × 2]&gt;\n 9 9     1     &lt;tibble [12 × 2]&gt;\n10 10    1     &lt;tibble [12 × 2]&gt;\n# … with 40 more rows\n\n# It is still a tibble\nclass(dat_nest)\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# But new data column is a list \nclass(dat_nest$data)\n\n[1] \"list\"\n\n# And each item within that list is actually a tibble!\ndat_nest$data[1]\n\n[[1]]\n# A tibble: 12 × 2\n   weight  Time\n    &lt;dbl&gt; &lt;dbl&gt;\n 1     42     0\n 2     51     2\n 3     59     4\n 4     64     6\n 5     76     8\n 6     93    10\n 7    106    12\n 8    125    14\n 9    149    16\n10    171    18\n11    199    20\n12    205    21\n\n\nNow, let’s use that nested data to run a linear model on the data column in each row (i.e., on each individual chick).\n\n# Linear model of weight over time for each chick\nlinear_model_results &lt;- ChickWeight |&gt;\n  group_by(Chick, Diet) |&gt;\n  nest() |&gt;\n  mutate(\n    fit = map(data, ~ lm(weight ~ Time, data = .)),\n    tidied = map(fit, tidy),\n    glanced = map(fit, glance)\n  ) |&gt; \n  unnest(tidied) \n\n# Print out the tibble results\nlinear_model_results\n\n# A tibble: 100 × 10\n# Groups:   Chick, Diet [50]\n   Chick Diet  data     fit    term    estim…¹ std.e…² stati…³  p.value glanced \n   &lt;ord&gt; &lt;fct&gt; &lt;list&gt;   &lt;list&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;list&gt;  \n 1 1     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   24.5    6.73     3.64 4.56e- 3 &lt;tibble&gt;\n 2 1     1     &lt;tibble&gt; &lt;lm&gt;   Time       7.99   0.524   15.3  2.97e- 8 &lt;tibble&gt;\n 3 2     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   24.7    4.93     5.01 5.26e- 4 &lt;tibble&gt;\n 4 2     1     &lt;tibble&gt; &lt;lm&gt;   Time       8.72   0.384   22.7  6.15e-10 &lt;tibble&gt;\n 5 3     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   23.2    5.08     4.56 1.04e- 3 &lt;tibble&gt;\n 6 3     1     &lt;tibble&gt; &lt;lm&gt;   Time       8.49   0.396   21.5  1.08e- 9 &lt;tibble&gt;\n 7 4     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   32.9    4.01     8.21 9.42e- 6 &lt;tibble&gt;\n 8 4     1     &lt;tibble&gt; &lt;lm&gt;   Time       6.09   0.312   19.5  2.70e- 9 &lt;tibble&gt;\n 9 5     1     &lt;tibble&gt; &lt;lm&gt;   (Inter…   16.9    7.56     2.24 4.93e- 2 &lt;tibble&gt;\n10 5     1     &lt;tibble&gt; &lt;lm&gt;   Time      10.1    0.588   17.1  9.88e- 9 &lt;tibble&gt;\n# … with 90 more rows, and abbreviated variable names ¹​estimate, ²​std.error,\n#   ³​statistic\n\n\n\n\n1.5.3 Calculating average linear growth rate\n\n# Calculate a per diet average growth rate from the slopes\ngrowth &lt;- linear_model_results |&gt; \n  filter(term == \"Time\") |&gt; \n  group_by(Diet) |&gt; \n  summarize(avg_slope = mean(estimate),\n            sd = sd(estimate))\n\n# Print out a pretty table with the results\ngrowth |&gt; \n  mutate(`Mean growth rate (g/day) ± std. dev.` = \n           glue::glue(\"{round(avg_slope, 2)} ± {round(sd, 2)}\")) |&gt; \n  mutate(`Diet name` =\n    case_when(Diet == 1 ~ \"Control diet\",\n              Diet == 2 ~ \"10% protein added\",\n              Diet == 3 ~ \"20% protein added\",\n              Diet == 4 ~ \"30% protein added\")) |&gt; \n  select(Diet, `Diet name`, `Mean growth rate (g/day) ± std. dev.`) |&gt; \n  kableExtra::kable(\"html\") |&gt; \n  kableExtra::kable_styling(\"striped\")\n\n\n\n\nDiet\nDiet name\nMean growth rate (g/day) ± std. dev.\n\n\n\n\n1\nControl diet\n5.85 ± 3.79\n\n\n2\n10% protein added\n8.61 ± 4.05\n\n\n3\n20% protein added\n11.42 ± 3.53\n\n\n4\n30% protein added\n9.52 ± 2.22\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice!\n\n\n\nWe likely won’t have time for this within the workshop. But on your own, try to see if you can test using an analysis of variance on the effect of diet on weight after 21 days.\n\n\n\n\nClick here for simple ANOVA example\n# One-way ANOVA of the effect of diet on Chick Weight after 21 days\naov_mod &lt;- ChickWeight |&gt; \n  filter(Time == 21) %&gt;%\n  aov(weight ~ Diet, data = .)\n\n# Print out the tibble results\ntidy(aov_mod)"
  },
  {
    "objectID": "tidyverse.html#introduction-to-data-wrangling",
    "href": "tidyverse.html#introduction-to-data-wrangling",
    "title": "1  Tidyverse",
    "section": "1.3 Introduction to data wrangling",
    "text": "1.3 Introduction to data wrangling\nThe package dplyr has a few core functions (or verbs) that are built to work on tidy data. We will check out a few briefly. But before we do, let’s introduce the sometimes confusing, but often useful, pipe operator.\n\n\n\n\n\n\nCore verbs\n\n\n\n\n\n\n1.3.0.1 Rows\n\nfilter() chooses rows based on column values.\nslice() chooses rows based on location.\narrange() changes the order of the rows.\n\n\n\n1.3.0.2 Columns\n\nselect() changes whether or not a column is included.\nrename() changes the name of columns.\nmutate() changes the values of columns and creates new columns.\nrelocate() changes the order of the columns.\n\n\n\n1.3.0.3 Groups of rows\n\nsummarise() collapses a group into a single row.\n\n\n\n\n\n\nPipe %&gt;%\nThe pipe operator is a more human-readable alternative to nesting function calls within each other. As the magrittr vignette explains, it makes function calls more naturally reflect the true order of operations. Here are the abstract examples of using a pipe.\n\nx %&gt;% f is equivalent to f(x)\nx %&gt;% f(y) is equivalent to f(x, y)\nx %&gt;% f %&gt;% g %&gt;% h is equivalent to h(g(f(x)))\n\nThe argument placeholder5\n\nx %&gt;% f(y, .) is equivalent to f(y, x)\nx %&gt;% f(y, z = .) is equivalent to f(y, z = x)\n\n\n\n\n\n\n\n|&gt; native pipe operator\n\n\n\n\n\nIn new versions of R (&gt; 4.1.0), there is now a native pipe operator (|&gt;) that comes within the syntax of R itself, rather than being loaded as a part of a separate package.\nAlthough they are largely the same, there are some meaningful differences between the two. These two articles from the tidyverse and Isabella Velásquez do a good job of explaining the differences relevant differences.\nI have started just using the native pipe (|&gt;) for almost all cases now. Trivially because it just looks cleaner. But also because using the native pipe allows you to use a pipe without explicitly loading the mattingr package. For example iris |&gt; head() just works, where if you instead had used the magrittr pipe (%&gt;%), but not loaded the magrittr package (or the whole tidverse), you would get an error.\nI will use the native pipe in almost all instances for the rest of this workshop.\n\n\n\n\n1.3.0.4 A fun example from R for data science\n\n# As a series of statements with intermediate objects\nfoo_foo &lt;- little_bunny()\nfoo_foo_1 &lt;- hop(foo_foo, through = forest)\nfoo_foo_2 &lt;- scoop(foo_foo_1, up = field_mice)\nfoo_foo_3 &lt;- bop(foo_foo_2, on = head)\n\n\n# As a bunch of nested function calls\nfoo_foo &lt;- bop(scoop(hop(little_bunny(), through = forest), up = field_mice), on = head)\n\n\n# With the pipe!\nfoo_foo &lt;- foo_foo |&gt;\n  hop(through = forest) |&gt;\n  scoop(up = field_mice) |&gt;\n  bop(on = head)\n\n\n\n\n\n\n\nThis last way of coding is much more human readable! Both the order of operations and which arguments belong to which function are clear. And it doesn’t clog up the global environment with a bunch of meaningless intermediate objects."
  },
  {
    "objectID": "data_viz.html#other-data-sets",
    "href": "data_viz.html#other-data-sets",
    "title": "2  Data visualization",
    "section": "2.10 Other data sets",
    "text": "2.10 Other data sets\nLet’s take some of these insights and use them to explore other data sets.\n\nPromptChick weight\n\n\n\n\n\n\n\n\nPractice\n\n\n\nFind a useful tip in Fundamentals of Data Visualization. Then take some data (built in data would be easiest), and try it out!\n\n\n\n\nWe looked at the ChickWeight data in the tidyverse section of this workshop. This is my attempt at making a polished figure that compares the growth rate of chicks across all four diet groups.\n\n\nCode\ndat &lt;- ChickWeight |&gt;\n  mutate(diet_name = case_when(Diet == 1 ~ \" Control diet\",\n                               Diet == 2 ~ \" 10% protein added\",\n                               Diet == 3 ~ \" 20% protein added\",\n                               Diet == 4 ~ \" 30% protein added\")) |&gt; \n  group_by(diet_name, Time) |&gt; \n  summarise(weight = mean(weight),\n            .groups = \"drop\")\n  \ndat |&gt;  \n  ggplot(aes(x = Time,\n             y = weight,\n             color = diet_name,\n             linetype = diet_name)) +\n  geom_line(linewidth = 1) +\n  geom_text(data = dat |&gt; \n              filter(Time == 21),\n            aes(label = diet_name),\n            hjust = 0,\n            nudge_x = 0) +\n  scale_color_manual(name = element_blank(), \n                     values = c(\"grey70\",\n                                \"orange\",\n                                \"grey70\",\n                                \"grey30\")) +\n  scale_y_continuous(limits = c(0, 300),\n                     expand = c(0, 0),\n                     minor_breaks = c(50, 150, 250),\n                     labels = c(\"0\", \"100g\", \"200g\", \"300g\"),\n                     name = \"Average chick mass\") +\n  scale_x_continuous(limits = c(0, 21),\n                     expand = c(0, 0),\n                     breaks = c(0, 7, 14, 21),\n                     labels = c(\"0\", \"7\", \"14\", \"21 days\"),\n                     name = element_blank()) +\n  scale_linetype_manual(values = c(1, 1, 1, 2)) +\n  coord_cartesian(clip = \"off\", xlim = c(0, 21)) +\n  cowplot::theme_minimal_hgrid() +\n  theme(legend.position = \"none\",\n        plot.margin = unit(c(1, 7, 1, 1), \"lines\"),\n        panel.grid.minor.y = element_line(color = \"grey80\", \n                                          linetype = 3),\n        axis.text.x = element_text(hjust = 0.2)) +\n  theme_light_axis()"
  },
  {
    "objectID": "data_viz.html#color",
    "href": "data_viz.html#color",
    "title": "2  Data visualization",
    "section": "2.7 Color",
    "text": "2.7 Color\nUse color to a tool highlight your results and the story of your data. But make sure not to overdo it.\nOkabe-Ito colors.\nTo continue with the penguins, if you are going to focus your data on the bigger Gentoo penguins, you could pick color to emphasize that\nAnd be sure to map salience to relevance. Don’t pick bad colors.\n\nLighten and Darken a palette for emphasis.\n\n# Lighten and desaturate the color palette\noi_colors_light &lt;- see::oi_colors() |&gt; \n  colorspace::lighten(amount = 0.6) |&gt; \n  colorspace::desaturate(amount = 0.8) \n\n# Darken the palette\noi_colors_dark &lt;- see::oi_colors() |&gt; \n  colorspace::darken(amount = 0.05)\n\n# Print colors\nscales::show_col(\n  c(oi_colors_light,\n    oi_colors_dark),\n  cex_label = 0.5,\n  ncol = 9, \n  borders = \"white\"\n)\n\n\n\n\n\n# Choose three colors -- two light and one dark from the palettes we created\ncolors &lt;- c(oi_colors_light[3:2], oi_colors_dark[5])\n\n# Plot with colors\npenguins |&gt; \n  ggplot(\n    aes(x = species,\n        y = body_mass_g,\n        fill = species)\n    ) +\n  geom_boxplot(\n    width = 0.5,\n    outlier.shape = NA,\n    #color = \"grey40\",\n    #alpha = 0.8\n    ) + \n  ggbeeswarm::geom_beeswarm(\n    shape = 21,\n    #color = \"grey60\",\n    #stroke = 0.3,\n    #alpha = 0.9\n    ) +\n  scale_y_continuous(\n    name = \"Body mass (g)\",\n    limits = c(0, 7000),\n    expand = c(0, 0.5)\n    ) +\n  scale_x_discrete(\n   name = element_blank()\n   ) +\n  scale_fill_manual(values = colors) +\n  cowplot::theme_minimal_hgrid() +\n  theme(legend.position = \"none\")\n\nWarning: Removed 2 rows containing non-finite values (`stat_boxplot()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nSimilar colors\n\n\n\nBecause the species is already encoded by the position along the x-axis, using a similar color for Chinstrap and Adelie penguins is okay, especially if we are just trying to emphasize the Gentoo penguins."
  },
  {
    "objectID": "data_viz.html#small-details",
    "href": "data_viz.html#small-details",
    "title": "2  Data visualization",
    "section": "2.8 Small details",
    "text": "2.8 Small details\nThere are many small things that you can do to emphasize your data.\nYou want to emphasize your data over all other elements of the plot. You want True black is particularly distracting for the eye. So it is best to\n\n# Plot with grey and alpha included\npenguins |&gt; \n  ggplot(\n    aes(x = species,\n        y = body_mass_g,\n        fill = species)\n    ) +\n  geom_boxplot(\n    width = 0.5,\n    outlier.shape = NA,\n    color = \"grey40\",\n    alpha = 0.8\n    ) + \n  ggbeeswarm::geom_beeswarm(\n    shape = 21,\n    color = \"grey60\",\n    stroke = 0.3,\n    alpha = 0.9\n    ) +\n  scale_y_continuous(\n    name = \"Body mass (g)\",\n    limits = c(0, 7000),\n    expand = c(0, 0.5)\n    ) +\n  scale_x_discrete(\n   name = element_blank()\n   ) +\n  scale_fill_manual(values = colors) +\n  cowplot::theme_minimal_hgrid() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n2.8.1 Custom theme\n\n# Define a custom theme\ntheme_light_axis &lt;- function(color = \"grey50\") {\n    theme(\n      axis.title = element_text(color = color),\n      axis.text = element_text(color = color)\n    )\n}\n\n\n# Plot with lighter axis text\npenguins |&gt; \n  ggplot(\n    aes(x = species,\n        y = body_mass_g,\n        fill = species)\n    ) +\n  geom_boxplot(\n    width = 0.5,\n    outlier.shape = NA,\n    color = \"grey40\",\n    alpha = 0.8\n    ) + \n  ggbeeswarm::geom_beeswarm(\n    shape = 21,\n    color = \"grey60\",\n    stroke = 0.3,\n    alpha = 0.9\n    ) +\n  scale_y_continuous(\n    name = \"Body mass (g)\",\n    limits = c(0, 7000),\n    expand = c(0, 0.5)\n    ) +\n  scale_x_discrete(\n   name = element_blank()\n   ) +\n  scale_fill_manual(values = colors) +\n  cowplot::theme_minimal_hgrid() +\n  theme(legend.position = \"none\") +\n  theme_light_axis()\n\n\n\n\n\n# Plot with lighter axis text\npenguins |&gt; \n  ggplot(\n    aes(x = species,\n        y = body_mass_g/1000,\n        fill = species)\n    ) +\n  geom_boxplot(\n    width = 0.5,\n    outlier.shape = NA,\n    color = \"grey40\",\n    alpha = 0.8\n    ) + \n  ggbeeswarm::geom_beeswarm(\n    shape = 21,\n    color = \"grey60\",\n    stroke = 0.3,\n    alpha = 0.9\n    ) +\n  annotate(\n    geom = \"text\",\n    x = 0.5,\n    y = c(0.2,2.2,4.2,6.2),\n    label = c(\"0 kg\", \"2 kg\", \"4 kg\", \"6 kg\"),\n    color = \"grey50\"\n  ) +\n  scale_y_continuous(\n    name = \"Body mass\",\n    limits = c(0, 7),\n    breaks = c(0, 2, 4, 6), \n    labels = c(\"0 kg\", \"2 kg\", \"4 kg\", \"6 kg\"),\n    expand = c(0, 0)\n    ) +\n  scale_x_discrete(\n   name = element_blank()\n   ) +\n  scale_fill_manual(values = colors) +\n  cowplot::theme_minimal_hgrid() +\n  theme(\n    legend.position = \"none\",\n    axis.text.y = element_blank(),\n    axis.ticks.length.y = unit(0, \"cm\")\n    ) +\n  theme_light_axis()"
  },
  {
    "objectID": "data_viz.html#other-data-to-plot",
    "href": "data_viz.html#other-data-to-plot",
    "title": "2  Data visualization",
    "section": "2.9 Other data to plot",
    "text": "2.9 Other data to plot\n\nIsland-Species distributionFlipper length vs. body mass\n\n\n\npenguins |&gt;  \n  group_by(species, island) |&gt; \n  tally() |&gt; \n  group_by(island) |&gt; \n  mutate(island_percent = n/sum(n)) |&gt; \n  ggplot(\n    aes(x = island_percent,\n        y = paste(island, \"island\"),\n        fill = species)\n    ) +\n  geom_col(width = 0.7) +\n  annotate(geom = \"text\", x = 0.075, y = 1, label = \"74%\",  color = \"grey80\", size = 6) +\n  annotate(geom = \"text\", x = 0.075, y = 2, label = \"55%\",  color = \"grey30\", size = 6) +\n  annotate(geom = \"text\", x = 0.075, y = 3, label = \"100%\", color = \"grey30\", size = 6) +\n  annotate(geom = \"text\", x = 0.925, y = 1, label = \"26%\",  color = \"grey30\", size = 6) +\n  annotate(geom = \"text\", x = 0.925, y = 2, label = \"45%\",  color = \"grey30\", size = 6) +\n  scale_x_continuous(limits = c(0, 1),\n                     expand = c(0, 0),\n                     position = \"top\",\n                     labels = scales::percent_format(accuracy = 1),\n                     name = element_blank()) + \n  scale_y_discrete(name = element_blank()) +\n  scale_fill_manual(values = colors,\n                    name = element_blank()) +\n  cowplot::theme_minimal_vgrid() +\n  theme_light_axis() +\n  theme(legend.position = \"bottom\",\n        plot.margin = margin(r = 20))\n\n\n\n\n\npenguins |&gt;  \n  group_by(species, island) |&gt; \n  tally() |&gt; \n  group_by(island) |&gt; \n  mutate(island_percent = n/sum(n)) |&gt; \n  ggplot(\n    aes(x = island_percent,\n        y = paste(island, \"island\"),\n        fill = species)\n    ) +\n  geom_col(width = 0.7) +\n  annotate(geom = \"text\", x = 0.15, y = 1, label = \"74% Gentoo   \",  color = \"grey80\", size = 5) +\n  annotate(geom = \"text\", x = 0.15, y = 2, label = \"55% Chinstrap\",  color = \"grey30\", size = 5) +\n  annotate(geom = \"text\", x = 0.15, y = 3, label = \"100% Adelie  \", color = \"grey30\", size = 5) +\n  annotate(geom = \"text\", x = 0.95, y = 1, label = \"26%\",  color = \"grey30\", size = 5) +\n  annotate(geom = \"text\", x = 0.95, y = 2, label = \"45%\",  color = \"grey30\", size = 5) +\n  scale_x_continuous(limits = c(0, 1),\n                     expand = c(0, 0),\n                     position = \"top\",\n                     labels = scales::percent_format(accuracy = 1),\n                     name = element_blank()) + \n  scale_y_discrete(name = element_blank()) +\n  scale_fill_manual(values = colors,\n                    name = element_blank()) +\n  cowplot::theme_minimal_vgrid() +\n  theme_light_axis() +\n  theme(legend.position = \"none\",\n        plot.margin = margin(r = 20))\n\n\n\n\n\n\n\npenguins |&gt;\n  ggplot(\n    aes(x = body_mass_g/1000,\n        y = flipper_length_mm/10,\n        color = species,\n        fill = species)\n    ) +\n  geom_point(shape = 21,\n             color = \"grey50\") +\n  geom_smooth(method = \"lm\",\n              linetype = 2,\n              color = \"grey50\",\n              fullrange = TRUE,\n              se = FALSE) +\n  scale_fill_manual(values = colors) +\n  cowplot::theme_minimal_grid() +\n  labs(x = \"Body mass (kg)\", y = \"Flipper length (cm)\") +\n  theme_light_axis() +\n  theme(legend.position = \"none\",\n        plot.margin = margin(r = 20),\n        strip.background = element_rect(fill = \"grey95\")) +\n  facet_wrap(~species) +\n  theme(panel.spacing = unit(0.75, \"cm\"))"
  },
  {
    "objectID": "data_viz.html#practice",
    "href": "data_viz.html#practice",
    "title": "2  Data visualization",
    "section": "2.9 Practice",
    "text": "2.9 Practice\n\nPromptIsland-Species distributionFlipper length vs. body mass\n\n\n\n\n\n\n\n\nPractice\n\n\n\nThere are two other questions I brainstormed from the beginning:\n\nWhat is the distribution of species on each island?\nDoes the correlation between flipper length and body mass vary between species?\n\nPick one. Or come up with a question yourself and make a graph!\nWhen you are done, compare what you have to the graph that I made in the other panels.\n\n\n\n\n\npenguins |&gt;  \n  group_by(species, island) |&gt; \n  tally() |&gt; \n  group_by(island) |&gt; \n  mutate(island_percent = n/sum(n)) |&gt; \n  ggplot(\n    aes(x = island_percent,\n        y = paste(island, \"island\"),\n        fill = species)\n    ) +\n  geom_col(width = 0.7) +\n  annotate(geom = \"text\", x = 0.15, y = 1, label = \"74% Gentoo   \",  color = \"grey80\", size = 5) +\n  annotate(geom = \"text\", x = 0.15, y = 2, label = \"55% Chinstrap\",  color = \"grey30\", size = 5) +\n  annotate(geom = \"text\", x = 0.15, y = 3, label = \"100% Adelie  \", color = \"grey30\", size = 5) +\n  annotate(geom = \"text\", x = 0.95, y = 1, label = \"26%\",  color = \"grey30\", size = 5) +\n  annotate(geom = \"text\", x = 0.95, y = 2, label = \"45%\",  color = \"grey30\", size = 5) +\n  scale_x_continuous(limits = c(0, 1),\n                     expand = c(0, 0),\n                     position = \"top\",\n                     labels = scales::percent_format(accuracy = 1),\n                     name = element_blank()) + \n  scale_y_discrete(name = element_blank()) +\n  scale_fill_manual(values = colors,\n                    name = element_blank()) +\n  cowplot::theme_minimal_vgrid() +\n  theme_light_axis() +\n  theme(legend.position = \"none\",\n        plot.margin = margin(r = 20))\n\n\n\n\n\n\n\npenguins |&gt;\n  ggplot(\n    aes(x = body_mass_g/1000,\n        y = flipper_length_mm/10,\n        color = species,\n        fill = species)\n    ) +\n  geom_point(shape = 21,\n             color = \"grey50\") +\n  geom_smooth(method = \"lm\",\n              linetype = 2,\n              color = \"grey50\",\n              fullrange = TRUE,\n              se = FALSE) +\n  scale_fill_manual(values = colors) +\n  cowplot::theme_minimal_grid() +\n  labs(x = \"Body mass (kg)\", y = \"Flipper length (cm)\") +\n  theme_light_axis() +\n  theme(legend.position = \"none\",\n        plot.margin = margin(r = 20),\n        strip.background = element_rect(fill = \"grey95\")) +\n  facet_wrap(~species) +\n  theme(panel.spacing = unit(0.75, \"cm\"))"
  },
  {
    "objectID": "data_viz.html#practice-2",
    "href": "data_viz.html#practice-2",
    "title": "2  Data visualization",
    "section": "2.11 Practice",
    "text": "2.11 Practice\nFind a useful tip in Fundamentals of Data Visualization. Then take some data (built in data would be easiest), and try it out!"
  }
]